---
layout: post
mathjax: true
catalog: true
comments: true
top-tags-list: true
header-img: "img/post-bg-universe.jpg"
header-mask: 0.4
title: ADMM与分布式计算
subtitle: 统计学习、ADMM求解算法与分布式计算
author: 乔林波
tags: [分布式计算, 统计学习, ADMM]
---



这篇博客主要是将 Stephen Boyd 2011年的文章《Distributed Optimization and Statistical Learning via the Alternating Direction Method of Multipliers》进行翻译和总结。


## 优化的一些基本算法思想
ADMM算法在1979年就被提出，但ADMM本身也是沿着一系列算法改进而来，并发展为可以支撑通用问题的分布式计算框架。在介绍ADMM之前，先要了解一些基本算法思想。

### Dual Ascent
对于凸函数的优化问题，对偶上升法的核心思想就是引入一个对偶变量，然后利用交替优化的思路，使得两者同时达到最优解。

\[ \begin{array}{lc} \min & f(x)\\\\ 
 s.t. & Ax = b \\\\ 
 \end{array} \Longrightarrow L(x, y) = f(x) + y^T(Ax - b) \overset{对偶函数（下界）}{\Longrightarrow} g(y) = \inf_x L(x, y) \]

在强对偶性的假设下，原问题和对偶问题可同时达到最优。

\[x^{\star} = \arg\min_x L(x, y^{\star})\]

因此，若对偶函数 $g(y)$ 可导，便可以利用梯度上升法，交替更新参数，使之与原函数同时收敛到最优。若对偶函数 $g(y)$ 不可微，则可以使用subgradient的方法类似求解。
迭代如下：

\[ \begin{split} 
x^{k + 1} : & =\arg\min_x L(x, y^k) \quad \text{($x$-最小化步)} \\\\ 
 y^{k + 1} : & = y^k + \alpha^k \nabla g(y) = y^k + \alpha^k (Ax^{k + 1} - b) \quad \text{(对偶变量更新，$\alpha^k$是步长)} \\\\ 
 \end{split} \]

在强对偶性假设下，即最小化原凸函数 $ p^* $ 等价于最大化对偶函数 $ d^* $，两者会同时达到最优解。更一般来说，一个凸函数的对偶函数其实就是原凸函数的一个下界。

这种内在的关联，可以将对原问题的求解转化为对 对偶问题的求解，从而将原问题中的参数约束条件大幅化简，这对优化求解是有十足的吸引力的。

在一定条件下，$x^k$ 和 $y^k$ 可以同时取到最优解，但是条件要求比较高：要求 $f(x)$ 严格凸，且 $\alpha$ 有恰当的取值。
然而，这两个条件不易满足，一般应用都不符合这条件，因此对偶上升法很少用于实际问题。

### Dual Decomposition
对偶分解是非常经典的优化方法，这种想法对后面的分布式优化方法影响较大。

对偶上升法有一个非常好的性质，当目标函数 $f$ 是可分的时候，整个问题可以拆解成多个关于子参数的子问题，拆分之后优化子问题，而后汇集整体更新。

而这对分布式并行处理来说是一个极好的消息。

\[ \begin{array}{ll} 
\min & f(x) = \displaystyle\sum^N_{i = 1} f_i(x_i), \\\\ 
s.t. & Ax = \displaystyle\sum^N_{i = 1} A_i x_i = b, \\\\ 
\end{array} \Longrightarrow L(x, y) = \sum^N_{i = 1}L_i(x_i, y) = \sum^N_{i = 1}(f_i(x_i) + y^TA_ix_i - \frac{1}{N}y^Tb) \]
其中，$x_i \in \mathbf{R}^{n_i}, x \in \mathbf{R}^n , n = \sum_i n_i$。

其中，关于 $x$ 的优化问题即可以拆分为多个子问题并行优化，而对偶变量更新不变，这对于变量维度较高时是非常有用的。

\[ \begin{split} x_i^{k + 1} : & =\arg\min_x L_i(x_i, y^k) \quad \text{(多个$x_i$并行最小化步)} \\\\ 
y^{k + 1} : & = y^k + \alpha^k \nabla g(y) = y^k + \alpha^k (Ax^{k + 1} - b) \quad \text{(汇集整体的$x$，然后对偶变量更新)} \\\\
 \end{split} \]

### Augmented Lagrangians and the Method of Multipliers
从上面可以看到对偶上升方法有良好的并行化潜力，但对于目标函数要求比较苛刻，为了获得更具一般性的假设条件，于是就有了Augmented Lagrangians方法，目的主要是放松对于 $f(x)$ 严格凸的假设和其他一些条件。

\[ L_{\rho}(x, y) = f(x) + y^T(Ax - b) + \frac{\rho}{2}\|Ax - b\|^2_2 \Longrightarrow
\begin{array}{ll}
 \min & f(x) + \frac{\rho}{2}\|Ax - b\|^2_2 \\\\ 
 s.t. & Ax = b
 \end{array} \]

增加了 $\rho/2 \|Ax - b\|$ 惩罚项的好处是使得对偶函数 $ g_{\rho}(y) = \inf_x L_{\rho}(x, y) $ 在更一般的条件下可导。

\[ \begin{split} x^{k+1} & = \arg\min_x L_{\rho}(x, y^k) \\\\ 
 y^{k+1} & = y^k + \rho(Ax^{k+1} - b) \\ \end{split} \]

即使 $f(x)$ 不是严格凸或者取值为 $+\infty$ ，该算法都有效，能够适用于更一般的问题。

虽然Augmented Lagrangians方法在算法假设条件和收敛性上有优势，但也不具备并行化的优势了。因为惩罚项的平方形式写成矩阵形式无法用之前那种分块形式，在 $x$ 子问题优化时无法分解为并行优化多个子参数 $x_i$。

## Alternating Direction Method of Multipliers(ADMM)
### ADMM算法概述
为了整合对偶上升法良好的可分解性与 ALM 优秀的收敛性质，人们就又提出了改进形式的算法，也就是ADMM。
ADMM主要是通过引入新变量，交叉更换方向来交替优化。

\[ \begin{array}{lc} \min & f(x) + g(z)\\\\ 
 s.t. & Ax + Bz = c \\\\
 \end{array} \Longrightarrow L_{\rho}(x, z, y) = f(x) + g(z) + y^T(Ax + Bz - c) + (\rho/2)\|Ax + Bz - c\|^2_2 \]

可以看到，ADMM 涵盖的问题较前面的对偶上升和ALM 更少，其思想就是把原变量、目标函数直接拆分，并且是最先开始就将拆开的变量分别看做是不同的变量 $x$ 和 $z$，同时约束条件也如此处理。

这样就使得后续优化过程的分解成为可能。于是ADMM的优化就变成了如下序贯型迭代：

\[ \begin{split} x^{k+1} & = \arg\min_x L_{\rho}(x, z^k, y^k) \\\\ 
z^{k+1} & = \arg\min_z L_{\rho}(x^{k+1}, z, y^k) \\\\ 
 y^{k+1} & = y^k + \rho(Ax^{k+1} + Bz^{k+1}- c) \end{split} \]

这种形式，two blocks，非常适合统计学习中的正则化问题：loss(x) + regulazition(z)。
ADMM 之所以可分，重点就在于使用一个 $z=g(x)$ 变量来简化问题，在实际应用中也可以根据需要构造一个 $z$ 出来，将原始变量解耦。

### ADMM算法性质和评价
#### 收敛性

关于收敛性，需要有两个假设条件：

$f$ 和 $g$ 分别是扩展的实数函数 $\mathbf{R}^n(\mathbf{R}^m) \rightarrow \mathbf{R}\bigcup \{+\infty\}$，且是closed、proper和convex的；
扩增的lagrangian函数 $L_0$ 有一个鞍点（saddle point）。
在此两个假设下，可以保证残差、目标函数、对偶变量的收敛性。


#### 停止准则

ADMM 求解过程中，最优条件需要注意的是：对偶残差（dual residuals）和初始残差（primal residuals）：

\[ \begin{split} s^{k + 1} & = \rho A^TB(z^{k+1} - z^k) \quad (dual \,\, residuals) \\\\ 
 r^{k + 1} & = Ax^{k+1} + Bz^{k+1} - c \quad (primal \,\, residuals) \end{split} \]

停止准则是求解算法比较重要的技术点，但实际应用中，一般都定在 primal residuals 和 dual residuals 足够小时停止迭代，阈值包含了绝对容忍度（absolute tolerance）和相对容忍度（relative tolerance）。但设置非常灵活和难把握，不少人吐槽这个停止准则不靠谱，并针对这个点做了自适应等很多研究工作：

\[ \begin{split} \|s^k\|_2 \leq \epsilon^{\text{dual}} & = \sqrt{n} \epsilon^{\text{abs}} + \epsilon^{\text{rel}} \|A^Ty^k\|_2 \\\\ 
 \|r^k\|_2 \leq \epsilon^{\text{pri}} & = \sqrt{p} \epsilon^{\text{abs}} + \epsilon^{\text{rel}}\max\\{\|Ax^k\|_2, \|Bz^k\|, \|c\|_2\\} \end{split} \]


### 分布式计算与Consensus

而ADMM 另一个值得称道的地方是，通过一致性优化问题与共享优化问题， ADMM 将各类应用与分布式和并行计算联系在了一起。

#### 全局变量一致性优化

所谓全局变量一致性优化问题，即目标函数根据数据分解成 $N$ 子目标函数（子系统），每个子系统和子数据都可以获得一个参数解 $x_i$，但是全局解只有一个 $z$，可以写成如下优化命题：

\[ \begin{array}{cl} \displaystyle \min_{x_1, \ldots, x_N} & \displaystyle \sum^N_{i = 1}f_i(x_i), x_i \in \mathbf{R}^n\\\\ 
 s.t. & x_i - z = 0 \end{array} \]

此时 $f_i: \mathbf{R}^n \rightarrow \mathbf{R} \bigcup \{+\infty\}$ 仍是凸函数，这里的损失函数是对数据的划分求和，而不是对参数空间进行划分，所有$x_i$维度都一样， $x_i, z \in \mathbf{R}^n$。
这种问题其实就是数据并行，希望从多个分块的数据集中获取相同的全局参数解。

然后，使用ADMM 迭代框架进行求解：

\[\begin{split} x_i^{k+1} & = \arg\min_x (f_i(x_i) + (y^k_i)^T(x_i - z^k) + (\rho/2) ||x_i - z ||)) \\\\ 
z^{k+1} & = \frac{1}{N} \sum^N_{i=1}( x^{k+1}_i + ( \frac{1}{\rho} y^k_i )) \\\\ 
y^{k+1}_i & = y^k_i + \rho(x^{k+1}_i - z^{k+1}) 
\end{split}
\]

其中，Lagrangian 函数为：

\[ \begin{array}{rl} L_{\rho}(x_1, \ldots, x_N, z, y) &= \displaystyle \sum^N_{i=1}(f_i(x_i) + y^T_i(x_i - z) + (\rho/2)\|x_i - z\|^2_2) \\\\ 
 s.t. & x_1 = \ldots = x_N \end{array} \]

对 $y$ 优化子问题和 $z$ 优化子问题的 $y_i^{k+1}$ 和 $z_i^{k+1}$ 分别求平均，可得$\bar{y}^{k+1}=0$，于是 $z$ 优化子问题步简化为 $z^{k+1} = \bar{x}^{k+1}$。

于是上述ADMM可进一步化简为如下形式：

\[ \begin{split} x_i^{k+1} & = \arg\min_x (f_i(x_i) + (y^k_i)^T(x_i - \bar{x}^k) + (\rho/2)\|x_i - \bar{x}^k\|^2_2)) \\\\ 
 y_i^{k+1} & = y_i^k + \rho(x_i^{k+1} - \bar{x}^{k+1}) \\ \end{split} \]

另外，对于全局一致性优化，也需要给出相应的终止迭代准则，与一般的ADMM类似，主要根据 primal和dual的residuals：

\begin{equation}
\begin{array}{ll}
||r^k || & = \sum^N_{i = 1} || x^k_i - \bar{x}^k ||  \\\\ 
||s^k || & = N \rho || \bar{x}^k_i - \bar{x}^{k-1}||
\end{array}
\end{equation}

至此，得到ADMM consensus 并行化的具体更新过程为：各个子数据分别并行求最小化，然后将各个子数据的解汇集起来求均值，整体更新对偶变量 $y^k$，然后再继续回带求最小值至收敛。



#### 局部变量一致性优化

全局一致性优化问题，其实是相对简单的数据并行。
现考虑更一般的一致性优化问题：在切分数据的同时，切分参数，各个子优化问题的参数不一定相同，可不相交，但也可能有部分重合。
确切的说，每个子目标函数 $f_i(x_i)$ 的参数维度不同，局部变量 $x_i \in \mathbf{R}^{n_i}$。局部变量所对应的不再是全局变量 $z$，而是全局变量中的一部分 $z_g$。令 $g = \mathcal{G}(i, \cdot)$，即将 $x_i$ 映射到$z$的某部位

\[ x_i = z_{\mathcal{G}(i, j)} = \hat{z}_j\]

如果有 $\mathcal{G}(i, j) = i$，局部变量一致性优化也就简化为全局一致性优化问题。

然后，纳入到 ADMM 框架求解：

\[ \begin{array}{lc} 
\min & \displaystyle \sum^N_{i = 1} f_i (x_i) + g(z), x_i \in \mathbf{R}^{n_i}\\\\ 
 s.t. & x_i - \hat{z}_i = 0, i = 1, \ldots N \\\\ 
\end{array} \]

\[ \Longrightarrow \]

\[ x_i^{k+1} = \arg\min_x (f_i(x_i) + (y^k_i)^T x_i (\rho/2) \|x_i - \hat{z}_i^k \|_2^2)) \]

\[ z^{k+1} = \arg \min_z (\sum^N_{i=1}(-(y^k_i)^T \hat{z}_i + (\rho/2) \|x^{k+1}_i - \hat{z}_i \|_2^2))) \]

\[ y_i^{k+1} = y_i^k + \rho(x_i^{k+1} - \hat{z}_i^{k+1}) \]

后续可同样做平均化处理， 与全局一致性优化类似，此时对于 $y$ 的取均值会为 0，优化过程则可以变为更简单的形式。


#### 切割大规模数据(正则化线性回归)

想要对大样本低维度数据进行处理，通过切割数据，利用并行计算是一个自然而然的途径。比如大规模正则化线性规划问题：

有观测阵 $A \in \mathbf{R}^{m \times n}$ 和响应值 $ b \in \mathbf{R}^m $，可以按数据维度对应切分：

\[ A = \begin{pmatrix} A_1\\\\  \vdots\\\\  A_N \\\\  \end{pmatrix} \quad b = \begin{pmatrix} b_1 \\\\  \vdots\\\\  b_N \\\\  \end{pmatrix} \]

于是原来带正则项的优化问题就可以按照数据分解到多个子系统上去分别优化，然后汇集起来，形成一个global consensus问题。

\[ \begin{array}{ll} \min & \displaystyle \sum^N_{i=1}l_i(A_ix_i - b_i) + r(z) \\\\  s.t. & x_i - z = 0, i = 1, \ldots, N \quad x_i, z \in \mathbf{R}^n \\ \end{array} \]


### 分布式计算与 Sharing

#### 共享问题 sharing

共享问题也是一个极具代表性的问题，与全局一致性优化相比，另多了一个共享的目标函数 $g$：

\[ \min_{x_1, \ldots, x_N} \,\, \sum^N_{i=1}f_i(x_i) + g(\sum^N_{i=1}x_i) \]

这种形式其实对应了两种实际应用问题，一是数据并行，优化各个损失函数，同时还要加上对全局数据的约束；二是优化多个子系统的部分变量，同时约束所有变量。

很明显共享问题也可以自然而然的纳入到ADMM框架中，形式如下：

\[ \begin{array}{ll} \min & \displaystyle\sum^N_{i=1}f_i(x_i) + g(\sum^N_{i=1}z_i) \\\\ 
 s.t. & x_i - z_i = 0, z_i \in \mathbf{R}^n, i = 1, \ldots, N, \\\\ 
\end{array} \]
\[ \Longrightarrow \]
\[ x_i^{k+1}  = \arg \min_{x_i} (f_i (x_i) + (\rho/2) || x_i - z_i^k + u_i^k ||) \]
\[ z^{k+1}  = \arg\min_z (g(\sum^N_{i = 1}z_i) + \rho/2\sum^N_{i = 1} ||z_i - x^{k+1}_i - u^k_i||) \]
\[ u_i^{k+1}  = u_i^k + x_i^{k+1} - z_i^{k+1} \]

上述形式中 $x$ 参数更新可以并行处理，而对于$z$的更新涉及最多对$Nn$个变量，这可通过平均方式来简化形式解。

对于$z$ 优化子问题，令$a_i = u^k_i + x^{k+1}_i$，于是$z$优化问题转化为：

\[ \begin{array}{ll} \min & g(N\bar{z}) + (\rho/2) \displaystyle \sum^N_{i=1}||z_i - a_i|| \\\\ 
 s.t. & \bar{z} = \frac{1}{N} \displaystyle \sum^N_{i=1}z_i \\ \end{array} \]

当$\bar{z}$固定时，那么后面的最优解（类似回归）为$z_i = a_i + \bar{z} - \bar{a}$，带入上式后于是后续优化就开始整体更新（均值化）

\[ \begin{split} x_i^{k+1} & = \arg\min_{x_i} (f_i(x_i) + (\rho/2)\|x_i - x_i^k + \bar{x}^k - \bar{z}^k + u^k\|^2_2)) \\\\ 
z^{k+1} & = \arg\min_z (g(N\bar{z}) + N\rho/2\|\bar{z} - \bar{x}^{k+1} - u^k\|^2_2) \\\\ 
u^{k+1} & = u_i^k + \bar{x}^{k+1} - \bar{z}^{k+1} \\ \end{split} \]

共享Sharing问题用来切分数据做并行化，也可以切分参数空间做并行化，特别是通过切分高维特征到低纬度中去求解，然后在合并更新，整个框架的收敛性还有保障，这是一个很美妙的事情。

而也与直观相一致的是，global consensus 问题与 sharing 问题是可以相互转化的，有证明条件是强对偶性存在，则可以同时达到最优，两者存在着很紧密的对偶关系。

#### 高维参数切分(正则化线性回归)

假设有一个观测阵 $A \in \mathbf{R}^{m \times n}$ 和响应观测 $b \in \mathbf{R}^n$ ，此时有 $n >> m$ ，那么要么就降维处理，要么就切分维度去处理。
此时 $A$ 矩阵按照参数空间切分（之前是按照数据维度切分），同时，假设正则项也可以切分为 $r(x) = \sum^N_{i = 1}r_i(x_i)$。

\begin{equation}
Ax = \sum^N_{i = 1}A_ix_i , \text{where   } A = [ A_1, \ldots, A_N ] , A_i \in \mathbf{R}^{m \times n_i}, x = (x_1, \ldots, x_N), x\in \mathbf{R}^{n_i}
\end{equation}

那么正则化线性回归问题形式就变成了

\[ \min \,\, l(\sum^N_{i = 1}A_ix_i - b) + \sum^N_{i = 1}r_i(x_i) \]

这个也就是sharing问题了：

\[ \begin{array}{lc} \min & l(\displaystyle \sum^N_{i=1}z_i - b) + \sum^N_{i=1}r_i(x_i) \\\\ 
s.t. & A_ix_i - z_i = 0, i = 1,\ldots, N \\\\ 
\end{array} \Longrightarrow \begin{split} x_i^{k+1} & = \arg\min_{x_i} (r_i(x_i) + (\rho/2)\|A_ix_i - A_ix_i^k + \overline{Ax}^k - \bar{z}^k + u^k\|^2_2)) \\\\ 
z^{k+1} & = \arg\min_z (l(N\bar{z} - b) + N\rho/2\|\bar{z} - \overline{Ax}^{k+1} - u^k\|^2_2) \\\\ 
u^{k+1} & = u_i^k + \overline{Ax}^{k+1} - \bar{z}^{k+1} \\ \end{split} \]


### 小结

在各类求解算法中，ADMM 收敛速度并不是最突出的，甚至被很多人吐槽是比较慢的。
一般需要数十次迭代，才可以得到一个大致可接受的结果，迭代次数更少的算法大有人在，比如快速高精度算法Newton法，内点法等。

不过，一般在机器学习实际应用问题中，loss 与 acc 并非严格一致的，参数解的精度高低对预测效果几乎没有什么影响，只需要在短时间内给出一个大致可行的结果，就可以应用预测了（预测效果基本一致）。

当然，在实际应用的时候，其实会将 ADMM 与其他快速算法结合起来，这样可以进一步提升 ADMM 的收敛速率和精度。

在分布式计算中，可以看到，在ADMM 框架下，consensus思想解决了切分大样本数据的并行化问题，而sharing思想解决了切分高维变量的并行化问题，并且可以有进一步细粒度的形式化可能。有了这个基础，大规模问题的求解与底层框架的融合也就是水到渠成的事情了。