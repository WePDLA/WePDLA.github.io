---
layout: post
mathjax: true
catalog: true
comments: true
top-tags-list: true
header-img: "img/post-bg-universe.jpg"
header-mask: 0.4
title: 深度学习学习笔记
subtitle: 深度学习出发点、实现与理论解释
author: 乔林波
tags: [深度学习,理论解释]
---

<center><iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width=330 height=86 src="//music.163.com/outchain/player?type=2&id=1329228822&auto=1&height=66"></iframe></center>

# 一、深度学习的有趣现象

## 1.1 特征抽象

2009年，在 Sparse Learning 研究社区，Bach Francis 小组的 Mairal Julien 发展出了 DL（Dictionary Learning）。

2012年，Hinton和他的学生Alex Krizhevsky设计了AlexNet，因其性能优异吸引了大量研究者关注 DL (Deep Learning)。

现在回过头来看，这两个 DL（Dictionary Learning 和 Deep Learning）有很有趣的联系。图1 和 图2 分别是Dictionary Learning学到的，和 DL （AlexNet）第一层学到的filters。

除第一层外，随着 DL 层数增加，深度加深时，DL 可以获得更为高层特征抽象，见 Matthew 在2013年的工作 Visualizing and Understanding Convolutional Networks。

DL 从底层特征到高层特征的逐步特征抽象能力，也带给深度学习 DL 以迁移学习的能力，形成预训练微调范式。
简单说来，通过 DL 在一个数据集上学习得到的低层次特征可以应用到另一个相似的数据集上。


{% include figure.html  width="488" src="/figures/20210721-DL/dictionary-learning.jpg" caption="图1：Dictionary Learning 从图像中学习得到的dictionary"%}

{% include figure.html  width="488" src="/figures/20210721-DL/AlexNet-filters.jpg" caption="图2：Deep Learning 网络第一层从图像中学习得到的filters"%}

{% include figure.html  width="488" src="/figures/20210721-DL/features.jpg" caption="图3：Deep Learning 从图像中学习得到的features"%}


## 1.2 Adversarial examples

深度学习在模型的鲁棒性和抗攻击性方面存在诸多潜在风险，被大家所熟知的是对抗样本的例子：给深度学习模型的输入添加少许扰动，可能带来预测结果的极大改变。

{% include figure.html  width="488" src="/figures/20210721-DL/adversarial-examples.jpg" caption="图4：Deep Learning 对抗样本"%}


## 1.3 Double descent curve.

深度学习的参数量远超样本数量，过参数具有记忆和泛化的能力，实验结果显示出一种超越传统机器学习“bias-variance”的现象：double descent。为什么会有这种现象呢？

详见： 
Zhang et al., Understanding deep learning requires rethinking generalization, ICLR, 2017；
Belkin et al., 2018. Hastie et al., 2019. Montanari and Mei, 2019.

{% include figure.html  width="768" src="/figures/20210721-DL/double-descent.jpg" caption="图5：Double descent"%}



## 1.4 Optimization landscape.

深度学习还缺乏公认的理论解释，而对深度学习的一类可视化工作给研究人员带来了很多直观的分析。

Li et al., Visualizing the loss landscape of neural nets, NeuIPS, 2018.
{% include figure.html  width="488" src="/figures/20210721-DL/dl-landscape.jpg" caption="图6：Deep Learning landscape"%}




# 二、深度学习实现（PyTorch）


## 2.1 Toy Example

以简单的神经网络为例子，引用[ Pytorch 官网 Tut](https://pytorch.org/tutorials/beginner/pytorch_with_examples.html#pytorch-nn) 中一个简单的神经网络，PyTorch 代码实现如下：

```python
# -*- coding: utf-8 -*-
import torch
import math


# Create Tensors to hold input and outputs.
x = torch.linspace(-math.pi, math.pi, 2000)
y = torch.sin(x)

# For this example, the output y is a linear function of (x, x^2, x^3), so
# we can consider it as a linear layer neural network. Let's prepare the
# tensor (x, x^2, x^3).
p = torch.tensor([1, 2, 3])
xx = x.unsqueeze(-1).pow(p)

# In the above code, x.unsqueeze(-1) has shape (2000, 1), and p has shape
# (3,), for this case, broadcasting semantics will apply to obtain a tensor
# of shape (2000, 3) 

# Use the nn package to define our model as a sequence of layers. nn.Sequential
# is a Module which contains other Modules, and applies them in sequence to
# produce its output. The Linear Module computes output from input using a
# linear function, and holds internal Tensors for its weight and bias.
# The Flatten layer flatens the output of the linear layer to a 1D tensor,
# to match the shape of `y`.
model = torch.nn.Sequential(
    torch.nn.Linear(3, 1),
    torch.nn.Flatten(0, 1)
)

# The nn package also contains definitions of popular loss functions; in this
# case we will use Mean Squared Error (MSE) as our loss function.
loss_fn = torch.nn.MSELoss(reduction='sum')

learning_rate = 1e-6
for t in range(2000):

    # Forward pass: compute predicted y by passing x to the model. Module objects
    # override the __call__ operator so you can call them like functions. When
    # doing so you pass a Tensor of input data to the Module and it produces
    # a Tensor of output data.
    y_pred = model(xx)

    # Compute and print loss. We pass Tensors containing the predicted and true
    # values of y, and the loss function returns a Tensor containing the
    # loss.
    loss = loss_fn(y_pred, y)
    if t % 100 == 99:
        print(t, loss.item())

    # Zero the gradients before running the backward pass.
    model.zero_grad()

    # Backward pass: compute gradient of the loss with respect to all the learnable
    # parameters of the model. Internally, the parameters of each Module are stored
    # in Tensors with requires_grad=True, so this call will compute gradients for
    # all learnable parameters in the model.
    loss.backward()

    # Update the weights using gradient descent. Each parameter is a Tensor, so
    # we can access its gradients like we did before.
    with torch.no_grad():
        for param in model.parameters():
            param -= learning_rate * param.grad
```



## 2.2 Backpropagation

代码实现中，参数更新是采用梯度下降方法:
```python
param -= learning_rate * param.grad
```
而神经网络参数的梯度 param.grad 没有解析解，直接写不出来，但直观上可以根据链式法则结合神经网络结构得到，也即是反向传播算法。
反向传播算法是目前深度学习框架主要采用的训练求解算法。
详细介绍参考：[http://neuralnetworksanddeeplearning.com/chap2.html](http://neuralnetworksanddeeplearning.com/chap2.html)

* Input $x$ : Set the corresponding activation $a^1$ for the input layer.

* Feedforward : For each $l=2,3,\cdots,L$ compute $z^l=w^l a^{l−1}+b^l$ and $a^l=\sigma(z^l)$

* Output error $\delta^L$ : Compute the vector $\delta^L=\nabla_a C \odot \sigma' (z^L)$

* Backpropagate the error : For each $l=L−1,L−2,\cdots,2$ compute $\delta^1=((w^{l+1})^T \delta^{l+1}) \odot \sigma'(z^l)$

* Output : The gradient of the cost function is given by $\frac{\partial C} {\partial w^l_{jk}}=a^{l−1}_k \delta^l_j$ and $\frac{\partial C} {\partial b^l_j}=\delta^l_j$


$w^l_{jk}$ denote the weight for the connection from the $k^{th}$ neuron in the $(l−1)^{th}$ layer to the $j^{th}$ neuron in the $l^{th}$ layer.

$b^l_j$ for the bias of the $j^{th}$ neuron in the $l^{th}$ layer. And we use $a^l_j$ for the activation of the $j^{th}$ neuron in the $l^{th}$ layer.

$s\odot t$ denote the elementwise product of the two vectors. Thus the components of $s\odot t$ are just $(s\odot t)_j=s_j*t_j$.




## 2.3 PyTorch 中 BP 算法实现

以 $ y = f(wx+b) $为例，前向和反向传播过程如图。

{% include figure.html  width="488" src="/figures/20210721-DL/bp.jpg" caption="图7：Forward and Backward"%}


### 前向过程 module(input)

forward做的事情就是将输入和参数灌入神经网络，使用 forward_call(*input, **kwargs) 调用模型，然后返回结果。

在这个例子中使用的Sequential来构建网络，Sequential继承自 Module，Sequential中每个layer如、class Linear(Module)、class Flatten(Module)都是继承自 Module。
Sequential Model 的调用也就直接遍历所有的layer，将前一层的结果作为后一层input，迭代所有的layer，将最终的结果作为模型的结果。

```python
#torch/nn/modules/module.py
class Module:
    ...
    def _call_impl(self, *input, **kwargs):
        forward_call = (self._slow_forward if torch._C._get_tracing_state() else self.forward)
        ...
            return forward_call(*input, **kwargs)
```

```python
#torch/nn/modules/container.py
class Sequential(Module):
    ...
    def forward(self, input):
        for module in self:
            input = module(input)
        return input
```


在前向过程中，PyTorch会构建计算图，每个节点用Variable表示，边表示由输入节点到输出节点的函数（torch.autograd.Function对象）。
以 Linear 层为例，经过python的错综复杂的内部调用进入 C++ 界面，在 C++ 代码做的事情核心是进行前向计算，并将结果wrap 封装成 PyObject， 然后返回。

```c++
// torch/csrc/autograd/generated/python_nn_functions.cpp
// linear
static PyObject * THPVariable_linear(PyObject* self_, PyObject* args, PyObject* kwargs)
{
  ...
  if (_r.isNone(3)) {
    // aten::linear(Tensor input, Tensor weight, Tensor? bias=None) -> Tensor
    auto dispatch_linear = [](const at::Tensor & input, const at::Tensor & weight, const c10::optional<at::Tensor> & bias) -> at::Tensor {
      pybind11::gil_scoped_release no_gil;
      return at::linear(input, weight, bias);
    };
    return wrap(dispatch_linear(_r.tensor(0), _r.tensor(1), _r.optionalTensor(2)));
  }

// aten/src/ATen/native/Linear.cpp
Tensor linear(const Tensor& input, const Tensor& weight, const c10::optional<Tensor>& bias_opt) {
  ...
    return at::addmm(*bias, input, weight.t());
}
```

然后 addmm 在前向过程中的核心是将 grad_fn 设置为对应的 ptr ，并维护构建计算图，以供后续使用。
```c++
// torch/csrc/autograd/generated/VariableType_0.cpp
at::Tensor addmm(c10::DispatchKeySet ks, const at::Tensor & self, const at::Tensor & mat1, const at::Tensor & mat2, const at::Scalar & beta, const at::Scalar & alpha) {
  auto& self_ = unpack(self, "self", 0);
  auto& mat1_ = unpack(mat1, "mat1", 1);
  auto& mat2_ = unpack(mat2, "mat2", 2);
  auto _any_requires_grad = compute_requires_grad( self, mat1, mat2 );
  ...
  std::shared_ptr<AddmmBackward0> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<AddmmBackward0>(new AddmmBackward0(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( self, mat1, mat2 ));
  ...
  if (grad_fn) {
      set_history(flatten_tensor_args( result ), grad_fn);
  }
  ...

// torch/csrc/autograd/functions/utils.h
inline void set_history(
    at::Tensor& variable,
    const std::shared_ptr<Node>& grad_fn) {
  AT_ASSERT(grad_fn);
  if (variable.defined()) {
    // If the codegen triggers this, you most likely want to add your newly added function
    // to the DONT_REQUIRE_DERIVATIVE list in tools/autograd/gen_variable_type.py
    TORCH_INTERNAL_ASSERT(isDifferentiableType(variable.scalar_type()));
    auto output_nr =
        grad_fn->add_input_metadata(variable);
    impl::set_gradient_edge(variable, {grad_fn, output_nr});
  } else {
    grad_fn->add_input_metadata(Node::undefined_input());
  }
}
```


### 反向过程 torch.Tensor.backward

在反向过程中，每个Function对象会调用.backward()函数计算输出对输入的梯度，然后将梯度传递给下一个Function对象。

根据函数定义 torch.Tensor.backward，或者 tensor.backward() 其实是调用 torch.autograd.backward，后者调用 Variable._execution_engine.run_backward。

而函数 Variable._execution_engine.run_backward 其实就是 torch._C 中的函数 _ImperativeEngine。

```python
def backward(self, gradient=None, retain_graph=None, create_graph=False):
    ...
    torch.autograd.backward(self, gradient, retain_graph, create_graph) 


def backward(tensors, grad_tensors=None, retain_graph=None, create_graph=False, grad_variables=None):
    ...
    Variable._execution_engine.run_backward(
        tensors, grad_tensors, retain_graph, create_graph,
        allow_unreachable=True)  # allow_unreachable flag


from torch._C import _ImperativeEngine as ImperativeEngine
Variable._execution_engine = ImperativeEngine()
```

然后，从 python 界面进入 C++ 界面。

函数 _ImperativeEngine 的实现在文件 torch/csrc/autograd/python_engine.cpp 中。

通过函数 PyModule_AddObject 将 (PyObject *)&THPEngineType 这个 object 加入到模块 module 中，并命名为 _ImperativeEngine。

代码上看，Variable._execution_engine.run_backward() 调用的就是 _ImperativeEngine().run_backward()。

从对象 THPEngineType 的定义看到， _ImperativeEngine().run_backward() 其实是 THPEngine_run_backward。

THPEngine_run_backward 经过处理后，调用 engine.execute()。
```c++
bool THPEngine_initModule(PyObject *module)
{
  PyModule_AddObject(module, "_ImperativeEngine", (PyObject *)&THPEngineType);
  set_default_engine_stub(get_python_engine);
  return true;
}

static struct PyMethodDef THPEngine_methods[] = {
  {(char*)"run_backward", (PyCFunction)(void(*)(void))THPEngine_run_backward, METH_VARARGS | METH_KEYWORDS, nullptr},
  {(char*)"queue_callback", (PyCFunction)THPEngine_queue_callback, METH_O, nullptr},
  {(char*)"is_checkpoint_valid", (PyCFunction)THPEngine_is_checkpoint_valid, METH_NOARGS, nullptr},
  {nullptr}
};

// torch/csrc/autograd/python_engine.cpp
// Implementation of torch._C._EngineBase.run_backward
PyObject *THPEngine_run_backward(THPEngine *self, PyObject *args, PyObject *kwargs)
{
    //...	
    variable_list outputs;
    {
        pybind11::gil_scoped_release no_gil;
        outputs = engine.execute(roots, grads, keep_graph, create_graph, output_edges);
    }
}
```


调用过程中，核心是对 grad 的计算更新，以及计算图的更新。

这里的 execute_with_graph_task() 在 python 部分启动后，在指定设备上具体执行，python 内 execute()向 cpu/device 内 execute() 的调用是直接调用的，并不会经过 dispatch。

但 cpu/device 内 execute() 调用 execute_with_graph_task() 则会又回到 python 内，然后再 python 内又调用 cpu/device 的 execute_with_graph_task()。


```c++
// torch/csrc/autograd/python_engine.cpp
variable_list PythonEngine::execute(
    const edge_list& roots,
    const variable_list& inputs,
    bool keep_graph,
    bool create_graph,
    bool accumulate_grad,
    const edge_list& outputs) {
    ...
    return Engine::execute(roots, inputs, keep_graph, create_graph, accumulate_grad, outputs);
    ...
}

// torch/csrc/autograd/engine.cpp
auto Engine::execute(const edge_list& roots,
                     const variable_list& inputs,
                     bool keep_graph,
                     bool create_graph,
                     const edge_list& outputs) -> variable_list {
  ...
  //... 准备 graph_task、graph_root
  ...
  return execute_with_graph_task(graph_task, graph_root)->wait();
}


std::shared_ptr<FutureVariableList> Engine::execute_with_graph_task(
    const std::shared_ptr<GraphTask>& graph_task,
    std::shared_ptr<Node> graph_root) {
      //...
      thread_main(graph_task, /* reentrant_thread */ true);
      //...
}

auto Engine::thread_main(
    const std::shared_ptr<GraphTask>& graph_task,
    bool reentrant_thread) -> void {
    //...
      if (task.fn_ && !local_graph_task->has_error_.load()) {
        AutoGradMode grad_mode(local_graph_task->grad_mode_);
        try {
          evaluate_function(local_graph_task, task.fn_.get(), task.inputs_);
        } catch (std::exception& e) {
          thread_on_exception(local_graph_task, task.fn_, e);
        }
      }
    //...
}

void Engine::evaluate_function(std::shared_ptr<GraphTask>& graph_task,
                               Node* func, 
                               InputBuffer& inputs){
    //...
     auto outputs = call_function(graph_task, func, inputs);
    //...
    
variable_list call_function(std::shared_ptr<GraphTask>& graph_task,
                            Node* func,
                            InputBuffer& inputBuffer) {
    auto& fn = *func;
    auto inputs = call_pre_hooks(fn, InputBuffer::variables(std::move(inputBuffer)));
    variable_list outputs = fn(std::move(inputs));

    if(has_post_hooks){
        // NOLINTNEXTLINE(bugprone-use-after-move)
        return call_post_hooks(fn, std::move(outputs), inputs);
    }
    return outputs;
}

```



## 2.5 optimizer


优化器主要是在模型训练阶段对模型可学习参数进行更新, 在训练过程中先调用 optimizer.zero_grad() 清空梯度，再调用 loss.backward() 反向传播，最后调用 optimizer.step()更新模型参数。

常用优化器有 SGD 及其变形改进版本，如 RMSprop，Adam，ASGD，Adadelta，Adagrad，AdamW，Adamax，SparseAdam，Rprop，LBFGS 等。

### 2.5.1 SGD

有了 $ \partial L/ \partial w$ 和 $ \partial L/ \partial b$ 之后，便可以使用 SGD 进行模型参数更新。


Repeat following steps until converged:
*	Calculate $ \partial L/ \partial w$, $ \partial L/ \partial b$ 
*	Update $ w = w - \eta*\partial L/ \partial w$
*	Update $ b = b - \eta*\partial L/ \partial b$ 


### 2.5.2 优化过程改进

朴素 SGD 在实际使用中，存在诸多问题，一般从三个方面改进优化训练过程。

* 正则化模型改进

为了改进模型性质，可通过对模型结构进行约束，添加正则项得到具有更好统计性质的模型。
如，$L_2$ 正则项 $\|w\|_2$。

模型参数更新从：优化目标函数：$ L $，更新参数：\[ w = w - \eta*\partial L/ \partial w\]

改变为：
优化目标函数：$ L' = L + 0.5* \lambda* |w |_2$, 更新参数：\[ w = w - \eta* \partial L/ \partial w -\eta* \lambda*w.\]


* 动量加速训练过程

为了加速训练过程收敛速度，可通过添加适当的动量来加速收敛。

例如，取动量 $\triangle w = -\partial L/ \partial w$, 权重更新为：

\[ w = w + \triangle w + m \triangle w_{pre}\]


* 训练过程稳定化技巧

例如，BERT中采用LayerNorm，公式如下：

\[\mu = \frac{1}{H}\sum\limits_{i=1}^H x_i\]

\[\sigma = \sqrt{\frac{1}{H}}\sum\limits_{i=1}^H (x_i-\mu)^2\]

\[
LN(x)=g\odot \frac{x-\mu}{\sqrt{\sigma^2+\epsilon}} + b
\tag{1}\label{LN}
\]

其中，$\odot$是element-wise相乘，$g$ 和 $b$ 是可学习的参数。



### 2.5.3 optimizer 实现


PyTorch中，Optimizer 是所有优化器的父类，它主要有如下公共方法:
* add_param_group(param_group): 添加模型可学习参数组；
* step(closure): 进行一次参数更新；
* zero_grad(): 清空上次迭代记录的梯度信息；
* state_dict(): 返回 dict 结构的参数状态；
* load_state_dict(state_dict): 加载 dict 结构的参数状态。




## 2.6 operator 实现

PyTorch 框架解析参考 [ezyang’s blog](http://blog.ezyang.com/2019/05/pytorch-internals/)

在 Forward 中使用的算子在代码中有直接的定义，在 Backward 中使用的算子是在编译过程中自动生成的，依据的定义详见 [derivatives.yaml](https://github.com/pytorch/pytorch/blob/master/tools/autograd/derivatives.yaml)。
把ATen的反向传播API转换成Function，在ATen的正向传播API中加入建图过程。

{% include figure.html  width="488" src="/figures/20210721-DL/operations.jpg" caption="图：PyTorch中 Operations"%}




## 2.7 profiler

torch.autograd.profiler 提供function级别的统计信息，
输出为包含 CPU 时间及占比，调用次数等信息（由于一个 kernel 可能还会调用其他 kernel，因此 Self CPU 指他本身所耗时间（不含其他 kernel 被调用所耗时间））：
```python
import torch
from torchvision.models import resnet18

x = torch.randn((1, 3, 224, 224), requires_grad=True)
model = resnet18()
with torch.autograd.profiler.profile() as prof:
    for _ in range(100):
        y = model(x)
        y = torch.sum(y)
        y.backward()
# NOTE: some columns were removed for brevity
print(prof.key_averages().table(sort_by="self_cpu_time_total"))
```


# 三、理解深度学习

接下来有时间了，再写目前学术社区对深度学习的理论理解。

