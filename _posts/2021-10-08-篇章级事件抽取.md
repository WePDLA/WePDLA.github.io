---
layout: post
mathjax: true
catalog: true
comments: true
top-tags-list: true
header-img: "img/post-bg-universe.jpg"
header-mask: 0.4
title: 篇章级事件抽取
subtitle: 中文金融领域篇章级事件抽取文章阅读笔记
author: 阚志刚
tags: [中文, 篇章级, 事件抽取]
---


事件抽取是信息抽取领域的一个重要的方向，其主要目的是从非结构化的自然语言文本中自动抽取结构化的事件信息。事件信息包括事件触发词、事件类型、事件元素和事件元素角色等，因此相较于实体抽取和关系抽取，事件抽取任务更为复杂。在很长一段时间内，事件抽取的研究工作围绕从句子中检测事件和抽取具体事件信息展开。然而现实中，一个事件通常会用文章的多个句子才能表述清楚，因此近年来研究者们逐渐开始关注篇章级文本中抽取事件信息。

# 一、DCFEE

## 1、简介

文章全名《DCFEE: A Document-level Chinese Financial Event Extraction System based on Automatically Labeled Training Data》，是一篇来自中科院自动化所的研究工作，发表在ACL2018[传送门](https://aclanthology.org/P18-4009.pdf)。相较于传统的句子级事件抽取工作，这应该是第一个在篇章范围内进行结构化事件信息抽取的工作。然而事件数据的标注是十分昂贵的，目前缺少篇章级的事件抽取数据。因此文章中提出了一种（金融领域的）数据自动标注方法，实现了一个篇章级事件抽取系统。

## 2、当前事件抽取存在的问题

* 如今的有监督的事件抽取工作大多依赖于人工标注的语料，目前中文金融领域缺少这样的语料。

* 当前的事件现有的事件抽取方法大多都是在句子层面（sentence level）做工作，而实际上一个事件往往分布在多个句子中。如下图所示：
{% include figure.html src="/figures/2021-10-08-DEE/dcfee-p1.png" caption="图1.1：由多个句子表述一个事件的例子"%}
传统的句子级抽取工作无法从这样的段落（文章）中准确地抽取事件信息。

为了解决这两个问题，本文：
* 提出了一种基于知识库的金融领域的事件数据自动标注系统。

* 提出了一个基于神经序列标注的文档级事件抽取模型，并形成了一个线上系统。

## 3、数据生成

### 3.1 数据来源
文中的数据由结构化和非结构化两部分数据组成：

#### 3.1.1 结构化的数据

结构化的数据来自金融专业人员提供的金融金融事件数据库，包含9种常见的财务事件类型，并以表格形式存储。这些包含关键事件论元的结构化数据是从金融专业人士的公告中总结出来的。以股权质押事件为例，如图1.2左侧所示，其中关键参数包括股东名称（名称）、质押机构（ORG）、质押股份数量（NUM）、质押开始日期（BEG）、质押结束日期（End）

#### 3.1.2 非结构化的数据

非结构化的数据来自互联网的官方公。

### 3.2 生成方法

数据生成过程包括句子级数据生成和文档级数据生成，总体过程如图2所示。

{% include figure.html src="/figures/2021-10-08-DEE/dcfee-p2.png" caption="图1.2：数据生成流程示意"%}

#### 3.2.1 句子级数据生成

在句子级数据生成的过程中，作者假设：公告中包含了大多数事件元素并且由触发词驱动的句子，就是事件提及。然后认为出现在事件提及中的事件元素，都与该事件提及中的事件有关系。针对每一类事件都定义了一个触发词字典，在生成的过程中，根据字典来确定触发词。

#### 3.2.2 文档级数据生成

在句子级数据标注的过程中，事件提及所在的样例为正样例，其他句子为负样例（包括只有事件元素但是没有触发词的那些句子）正样例和负样例组合成为文档级的数据。

#### 3.2.3 数据生成过程中的tips

* 缩小搜索空间：通过结合公开数据搜索关键事件元素来缩小候选事件元素的搜索范围。
* 正则化表示：当事件数据库与来自网上的公告在表示上有出入时，正则表达式可以有效地提高标注数据的召回率（recall）。
* 规则：一些任务驱动的规则可以用来服务于数据生成。比如质押起始时间是2017年2月23日，终止时间是2018年2月23日，则可以将句子中的12个月标注为其所对应的标签值。

## 4、事件抽取

篇章级事件抽取系统的总体结构如图1.3所示。总体而言该模型先进行句子级的事件抽取，然后再进行文档级的事件元素补充。

{% include figure.html src="/figures/2021-10-08-DEE/dcfee-p3.png" caption="图1.3： DCFEE模型的框架。"%}

### 4.1 句子级事件抽取

图1.3中黄色方框中的部分为句子级事件抽取（SEE）模型部分，模型上使用双向LSTM(BiLSTM)来捕捉单词特征，而后使用CRF来做序列标注工作。

### 4.2 文档级事件抽取

文档级事件抽取（DEE）包括关键事件检测和事件元素填充。

#### 4.2.1 关键事件检测

将SEE中得到的触发词和事件元素（蓝色方框）与SEE部分原始的单词表示（红色方框）拼接，通过一个CNN搭建二分类网络，判断这个句子是不是关键事件。

#### 4.2.2 事件元素填充

从关键句子的周围的句子中找事件元素来填补关键事件中的不足的事件元素。

## 5、总结

这个工作是较早的篇章级事件抽取工作，它的整体思路是先进行句子级的事件抽取，然后找出描述事件的关键句子。以这个句子为中心，用周围句子的SEE结果补全对事件进行事件元素补全。这个工作几乎是开创了从多个句子中抽取事件信息的先河，有很多优点。但也存在一个严重的缺陷：不能处理一篇文档中有多个事件的情况。另外，先做SEE再做元素补全的这种方法很显然将文章中的句子分开编码处理，因此忽视了对全局信息的捕捉。

# 二、Doc2EDAG

# 1、 简介

文章全名《Doc2EDAG: An End-to-End Document-level Framework for Chinese Financial Event Extraction》，工作来自清华大学跨学科信息科学研究所和微软研究院，发表在ACL2019，[文章链接](https://aclanthology.org/D19-1032.pdf)。文章认为现有的篇章级事抽取工作（DCFEE：那不就是我吗？？）需要预先定义触发词字典和其他人工的工作，无法处理一篇文章中包含多个事件的情况，存在一定的局限性。该文章1）将事件抽取工作由原先的序列标注问题转变成为一个事件元素路径扩展的问题，能够直接从文档中生成事件表格；2）重新形式化了文档级的事件抽取任务，在这种新的模式中文档级事件抽取不需要触发词，避免了错误传播的问题；3）新建了一个中文金融领域的文档级事件抽取数据集，该数据集的数据量是DCFEE的数据集10倍左右。该数据集中，一个文档中包含多个事件的比例（30%）比DCFEE的文档级事件数据集(3%)更高。

# 2、 事件表格

为了解决一篇文章中存在多个事件的难题，这个研究工作将文章中的事件信息用事件表格表示。图2.1描述了如何形成事件表格。
{% include figure.html src="/figures/2021-10-08-DEE/doc2edag-p1.png" caption="图2.1 篇章级事件信息的事件表格表示。"%}

图2.1中，右上角的表格即为事件表。在本文中，每个事件类型都有它特有的事件表格，它们的表头是这个事件类型包含的事件元素，顺序按出现频率由高到低排列。表格中的一行就是一个事件。

# 3、 模型

本文将文档级事件抽取分为三个步骤：实体抽取、事件检测、事件表格填写。本文的工作本质上是将event record转变为“基于实体的有向无环图(entity-based directed acyclic graph, EDAG)”，使得模型能够自动地生成这种图。模型的结构如图2.2所示。

{% include figure.html src="/figures/2021-10-08-DEE/doc2edag-p2.png" caption="图2.2 Doc2edag模型的结构示意图。"%}

## 3.1 输入

以查表的方式获得字的embedding。句子最多包含128个字，文档最多包含64个句子。

## 3.2 实体识别 

以双向LSTM加上CRF的方式抽取实体，实体的标签就是它们对应的事件元素角色，标签以BIO形式标注。

## 3.3 文档级实体编码

进行文档级实体编码的目的是使得实体编码和句子编码能够在一定程度上捕捉到文章的全局信息。文档级实体编码主要是将原始句子编码和原始实体编码串联成为一个序列，送入一个transforme，得到的向量序列即为包含了整个文档信息的句子编码和实体编码。原始实体编码是在实体包含的字符对应的字符向量上加一个max pooling来获取实体编码。原始句子编码是在句子对应字符的字符向量上加上一个max pooling来获取句子编码。

## 3.4 EDAG生成

这里对应着“事件检测”、“事件表格填写”（或者称之为“路径扩展”）两个过程：

### 3.4.1 EDAG建立

{% include figure.html src="/figures/2021-10-08-DEE/doc2edag-p3.png" caption="图2.3 EDAG的生成示意图。"%}

对于每一个事件类型，按照事件角色出现的频率先建立一个事件角色的序列，而后根据这个顺序去判断每一个事件角色是否在当前的文本中存在，若存在则抽取该实体，否则则置为NA，最终形成一个list。将这个list填到类似图2.3中的path expanding中去。由此，事件抽取任务就变成根据事先排列好的事件角色中的每一个角色，判断实体识别得到的实体是否属于该角色。

### 3.4.2 记忆机制

如图2.4所示，m为一个记忆矩阵，它使用句子编码和已经处理完了的事件角色对应的实体编码表示当前文档信息和已有的路径。这样做的目的是让已有的路径信息能够帮助模型判断当前事件元素是哪个实体。

{% include figure.html src="/figures/2021-10-08-DEE/doc2edag-p4.png" caption="图2.4 元素角色分派示意图。"%}

### 3.4.3 路径扩展

如图2.5所示，将记忆矩阵m与实体句子ed拼接,加上当前需要判断的事件角色的信息，送入一个transformer得到一个包含更多信息的实体特征，将这个实体特征送入一个线性的二分类网络判断它是否属于当前角色。

{% include figure.html src="/figures/2021-10-08-DEE/doc2edag-p5.png" caption="图2.5 路径扩展分类器。"%}

### 3.4.4 loss

记实体识别、事件（探测）分类、路径扩展所有子任务 的loss之和分别为Le r , Lt r , Ldag,则最终的loss为:
\[ L_{all} = \lambda_1 L_{er} + \lambda_2 L_{tr} + \lambda_3 L_{dag}, \]
本文中 $\lambda_1 \lambda_2  \lambda_3$ 分别为为0.05，0.95，0.95。

### 3.4.5 tips

* 在训练过程中使用scheduled sampling，在训练早期该方法主要使用目标序列中的真实元素作为解码器输入，可以将模型从随机初始化的状态快速引导至一个合理的状态。随着训练的进行，该方法会逐渐更多地使用生成的元素作为解码器输入，以解决数据分布不一致的问题。

* 由于在本文模型中，假的正样本比假的负样本危害更大，因此作者给负样本加上了一个大于1的权重，以增加模型将负样本分成正样本时受到的惩罚。

## 3.5 总结

文章创新性地将文章中的事件信息用事件表格的形式来表示，用路径扩展的方法解决了一篇文章中存在多个相同类型事件的问题。文章中先做NER任务，然后再将实体和句子编码串联起来进一步进行特征学习的方法在一定程度上使得实体编码中蕴含了文章的全局信息。

不过个人认为这篇文章的方法还有可以改进的地方：比如实体识别使用的编码还是没有文章的全局信息；先判断事件类型再进行事件元素抽取的框架会导致错误传播；文章中应该存在很多共指实体，这些共指实体会导致模型额外扩展出一些路径。

# 三、GIT

## 1、简介

文章全名《Document-level Event Extraction via Heterogeneous Graph-based Interaction Model with a Tracker》，是来自北京大学计算语言学重点实验室、鹏程实验室、字节跳动的工作。这篇文章发表在ACL2021。文章的动机是文档中多个事件之间应当存在潜在关系可以利用。

## 2、模型

模型的框架如图3.1所示。

{% include figure.html src="/figures/2021-10-08-DEE/git-overview.png" caption="图3.1 GIT模型的框架。"%}

### 2.1 编码

先获取词向量，再使用transformer进一步进行编码。

### 2.2 实体抽取

使用CRF进行命名实体抽取。

### 2.3 异构交互图构建

异构交互图如下面两张图所示，其中图3.2是异构交互图，图3.3体现了异构交互图中各线段的含义。

{% include figure.html src="/figures/2021-10-08-DEE/异构交互图.png" caption="图3.2 异构交互图示例。"%}

{% include figure.html src="/figures/2021-10-08-DEE/异构交互图-信息.png" caption="图3.3 异构交互图中的元素解释。"%}

图中的节点包括实体节点和句子节点，边包括“句子--句子边”，“句子--实体边”，“内部实体-实体边”和“外部实体--实体边”。使用GCN对异构交互图中的信息进行编码。

### 2.4 事件检测

将图中的句子节点表示组合起来，形成文档的表示。对文档表示进行多标签分类，判断文章中有哪些事件。

### 2.5 事件记录抽取

事件记录抽取的流程如图3.4所示。

{% include figure.html src="/figures/2021-10-08-DEE/git-事件记录抽取.png" caption="图3.4 事件记录抽取流程。"%}

模型在每个树节点对每个实体二分类，判断实体是否是对应的事件元素。本文中设置了全局记忆矩阵（追踪器），作用是当模型在预测事件元素的时候，模型能够从追踪器中查询到其他事件记录中的有用信息。追踪器是使用transformer对每一个事件记录进行编码，输入包括事件记录中相关实体的表示E、由句子表示组成的文档表示S、由事件元素角色组成事件记录路径表示G以及由$E_{1,2,...,n}$送入LSTM得到的全局记忆G组成。


## 3、 总结

这篇文章很有意思，它认为在抽取文章中事件的时候，文章中其他的事件记录也会帮助到该条事件的抽取，所以设计了一个追踪器记录了已有事件信息的追踪器。 


# 四、 DE-PNN

## 1、 简介

文章全名《Document-level Event Extraction via Parallel Prediction Networks》，[传送门](https://aclanthology.org/2021.acl-long.492.pdf)。这个工作来自中科院，发表在ACL2021上，是一个尝试用生成的方法来做篇章级事件抽取的工作。上面的两种抽取式的方法（Doc2edag、GIT）将篇章级事件抽取任务表示为填充事件记录表，按照一定顺序将事件元素填写到表格中的对应位置。本文指出这种方法存在一个局限性：在抽取当前事件元素的时候没有考虑到后续元素的信息。而本文提出来的生成式抽取方法能够避免这样的问题。个人认为这篇文章写得非常好，详细阐述了篇章级事件抽取与句子级事件抽取工作的区别和目前存在的难点，对于我这样刚入门的菜鸟很友好。

## 2、 方法

### 2.1 问题形式化

给定一个包含了 $ N_s $ 个句子的文档作为输入，文档表示为：
\[ D={S_{i}}_{i=1}^{N_{s}}. \]
篇章级事件抽取的目的是抽取一个或者多个结构化的事件信息：
\[ Y={y_i}_{i=1}^k. \]
其中，表示第t个事件类型的事件$ y_i^t $包含了一角色$ (r_i^1, r_i^2, ..., r_i^n) $ ，它们将由事件元素$ (a_i^1, a_i^2, ..., a_i^n) $ 来填充。k是文档中包含的事件的数量，$n$是预先定义的事件类型t中事件角色的数量。

### 2.2 候选元素识别

首先对文档中的*每一个句子（逐句）*进行编码得到 $ [w_{i,1}, w_{i,2}, ..., w_{i,l}] $其中*l*是句子的长度。然后用transformer作为编码器，得到第i个句子有上下文感知的编码$ C_i $：
\[ C_i = Transformer-1(S_i)\]
对于文档中的每个句子做此操作就得到$ {C_i}_{i=1}^{N_{s}} $。接着将句子级候选元素识别作为经典的序列标注任务来完成。最后从第 *i* 个句子中得到候选元素：
\[ A = {a_i}_{i=1}^{N_a} \]
其中N_a是识别到的候选元素的数量。

### 2.3 文档级Encoder

为了获取文档级的信息，本文设计了文档级Encoder对候选元素编码和句子编码进行进一步编码。候选元素的编码表示$ c_i^a $是在候选元素包含的token的编码$ [c_{i,j}, ..., c_{i,j}] $ 上做max-pooling得到。句子编码 $ c_i^s $ 同理。文档级Encoder的过程如下：
\[ [H^a; H^s]=Transformer-2(c_1^a...c_{N_a}^a; c_1^s...c_{N_a}^s) \]
$ H^a $和$ H^s $ 分别是encode之后的候选元素编码和句子编码。作者对每一种事件类型使用一个二分类器来判断文章中是否有该事件。

### 2.4 多粒度Decoder(Multi-Granularity Decoder)

多粒度Decoder包括三个组成部分：事件解码器（event decoder）、角色解码器（role decoder）以及事件-角色解码器（event-to-role decoder）。使用解码器可以并行地抽取事件、元素、以及事件-角色的信息。

#### 2.4.1  事件解码器

在进行事件解码之前首先要告诉模型文档中有多少事件，在本文中作者假设文档中有m个事件，将其作为超参数，模型后续会生成m个事件。这里使用m个可学习的向量 $ Q^{event} \in \mathbb{R}^{m\times d} $ 作为事件解码器的输入。解码器中包含多头自注意力机制和一个多头交互注意力机制（a multi-head cross-attention mechanism）来将文档表示 $ H^s $ 整合到事件序列 $ Q^ {event} $ 中。在形式上，这m个事件序列被解码成m个输出编码：
\[ H^{event}=Event-Decoder(Q^{event},H^s), \]
其中 $ H^{event} \in \mathbb{R}^{m \times d} $

#### 2.4.2 元素解码器

事件元素解码器是用来并行地抽取一个事件中所有的事件元素。解码过程与事件解码器的过程类似。对于一个t类型的事件，它的事件元素角色为 $ (r_1, r_2, ..., r_n) $。使用n个可学习的向量$Q^{role} \in \mathbb{R}^{n\times d} $ 作为元素解码器的输入。解码器中包含多头自注意力机制和一个多头交互注意力机制来将候选事件元素表示 $ H^a $ 整合到事件序列 $ Q^ {event} $ 中。在形式上，这n个元素序列被解成n个输出编码：
\[ H^{role}=Role-Decoder(Q^{role},H^a) \] ，其中 $ H^{role} \in \mathbb{R}^{n \times d} $

#### 2.4.3 事件-元素解码器

(这里具体是怎么实现的文章没说，等我看看代码之后再更新)
\[ H^{e2r}=Event2Role-Decoder(H^{role},H^{event}), \] 
其中 $ H^{e2r} \in \mathbb{R}^{m \times n \times d}. $

### 2.5 事件预测

首先将m个事件序列$ H^{event} $ 送入前馈网络中，以判断事件是否发生：
\[ P^{evnet}=softmax(H^{event}W_e), \]
其中 $ W_{e} \in \mathbb{R}^{d \times 2}. $ 然后，对于每一个事件，判断它里面的每一个事件元素角色是候选元素中的哪一个：
\[ P^{role}=softmax(tanh(H^{e2r}W_1 + H^{a}W_2) \cdot v_1), \] 其中 $P^{role} \in \mathbb{R}^{m \times n \times (N_a^{'}+1)}. $ 最后，得到了m个事件 $ \hat{Y}=(\hat{Y}_1, \hat{Y}_2, ..., \hat{Y}_m) $，其中每一个事件包含n个事件元素角色 $ \hat{Y}_i=(P^1_i, P^2_i, ..., P^n_i) $ , $P_i^j=P^(role)[i,j,:] \in \mathbb{R}^{N_a^{'}+1} $ 。这里的$ P_i^j $的物理含义：各个候选元素是第i个事件中第j个角色的概率。

### 2.6 Matching Loss

为了让抽取出来的事件序列能够跟真实的事件序列相比较，本问题提出了一种用于篇章级事件抽取的损失函数计算方法————Matching Loss。

首先，需要找到预测事件序列 $\hat{Y}$的所有排序空间中找到和真实事件序列 $Y$ 之间的最小差异的排序：
\[ \hat{\sigma} = \underset{\sigma \in \Pi(m)} {argmax} \sum_i^m C_{match}(\hat{Y}_{\sigma(i),Y_i})\] ，其中$ \Pi(m) $是m个事件的所有排列组合空间。$C_{match}(\hat{Y}_{\sigma(i),Y_i}) $ 是真实值$Y$和第 $ \sigma(i) $ 个预测值 $\hat{Y}$ 之间的差异。

$C_{match}(\hat{Y}_{\sigma(i),Y_i}) $的定义：

\[ C_{match}(\hat{Y}_{\sigma(i),Y_i})=\mathbb{1}_{judge_{1} \neq \phi} \sum_{j=1}^n P_{\sigma(i)}^j (r_i^j), \] 其中 $ judge_i $ 是第$i$个事件类型的预测结果。

最终的loss为：
\[ L(\hat{Y},Y)=\sum_{i=1}^m \mathbb{1}_{judge_{1} \neq \phi}[\sum_{j=1}^n -log P_{\hat{\sigma}(i)}^j(r_i^j)] \] ，其中 $ \hat{\sigma} $ 是与真实值差异最小那个排列。

### 2.7 最终loss

\[ L_{aLL} = \lambda_{1}L_(see) + \lambda_{2}L_(ec) + \lambda_{3}L_(Y, \hat{Y}), \]
 其中$ L_{see} $ 是事件类型分类任务的交叉熵， $ L_{ea}$ 是句子级候选事件元素抽取的交叉熵。

## 3、 总结

本文的思路很好，用解码的方法解决了路径扩展方法中可能存在的错误传播问题。但是固定了事件的数量，这又会带来新的误差。
