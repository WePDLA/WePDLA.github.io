---
layout: post
mathjax: true
catalog: true
comments: true
top-tags-list: true
header-img: "img/post-bg-universe.jpg"
header-mask: 0.4
title: 深度学习学习笔记
subtitle: 深度学习出发点、实现与理论解释
author: 乔林波
tags: [深度学习,理论解释]
---

<center><iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width=330 height=86 src="//music.163.com/outchain/player?type=2&id=1329228822&auto=1&height=66"></iframe></center>

# 一、深度学习的有趣现象

## 1.1 特征抽象

2009年，在 Sparse Learning 研究社区，Bach Francis 小组的 Mairal Julien 发展出了 DL（Dictionary Learning）。

2012年，Hinton和他的学生Alex Krizhevsky设计了AlexNet，因其性能优异吸引了大量研究者关注 DL (Deep Learning)。

现在回过头来看，这两个 DL（Dictionary Learning 和 Deep Learning）有很有趣的联系。图1 和 图2 分别是Dictionary Learning学到的，和 DL （AlexNet）第一层学到的filters。

除第一层外，随着 DL 层数增加，深度加深时，DL 可以获得更为高层特征抽象，见 Matthew 在2013年的工作 Visualizing and Understanding Convolutional Networks。

DL 从底层特征到高层特征的逐步特征抽象能力，也带给深度学习 DL 以迁移学习的能力，形成预训练微调范式。
简单说来，通过 DL 在一个数据集上学习得到的低层次特征可以应用到另一个相似的数据集上。


{% include figure.html  width="488" src="/figures/20210721-DL/dictionary-learning.jpg" caption="图1：Dictionary Learning 从图像中学习得到的dictionary"%}

{% include figure.html  width="488" src="/figures/20210721-DL/AlexNet-filters.jpg" caption="图2：Deep Learning 网络第一层从图像中学习得到的filters"%}

{% include figure.html  width="488" src="/figures/20210721-DL/features.jpg" caption="图3：Deep Learning 从图像中学习得到的features"%}


## 1.2 Adversarial examples

给深度学习模型的输入添加少许扰动，可能带来预测结果的极大改变。

{% include figure.html  width="488" src="/figures/20210721-DL/adversarial-examples.jpg" caption="图4：Deep Learning 对抗样本"%}


## 1.3 Double descent curve.

深度学习的参数量远超样本数量，过参数具有记忆和泛化的能力，并且拥有超越传统机器学习“bias-variance”的现象：double descent。

详见： 
Zhang et al., Understanding deep learning requires rethinking generalization, ICLR, 2017；
Belkin et al., 2018. Hastie et al., 2019. Montanari and Mei, 2019.

{% include figure.html  width="768" src="/figures/20210721-DL/double-descent.jpg" caption="图5：Double descent"%}



## 1.4 Optimization landscape.

对深度学习的 loss landscape 可视化，可以看到不同网络模型结构带来的 landscape 的不同。

详见：
Li et al., Visualizing the loss landscape of neural nets, NeuIPS, 2018.

{% include figure.html  width="488" src="/figures/20210721-DL/dl-landscape.jpg" caption="图6：Deep Learning landscape"%}




# 二、深度学习实现（PyTorch）


## 2.1 Toy Example

一个简单的深度神经网络，例如：线性回归模型$\min_\mu l(Y, X\mu)$，用 PyTorch 代码实现如下：

```python
import math
import torch
import torch.nn as nn 
import torch.optim as optim
n = 100000
p= 512
x = torch.randn(n,p)
beta = torch.randn(p,1)/math.sqrt(p)
y = torch.mm(X,beta)+0.5 * torch.randn(n,1)
class LinearReg(nn.Module): 
  def __init__(self):
    super(LinearReg,self).__init___() 
    self.linear = torch.nn.Linear(p,1) 
  def forward(self,x) :
    y_pred = self.linear(x) 
    return y_pred

from torch.optim.Ir_scheduler import MultiStepLR
batch_sz = 128
epochs = 9
model = LinearReg()
optimizer = optim.SGD(model.parameters(),lr=0.01,momentum=0.9,weight_decay=5e-4)
scheduler = MultiStepLR(optimizer,milestones=[3,6],gamma=0.2)
Loss = nn.MSELOSS()
for epoch in range(epochs):
  scheduler.step()
  for idx in range(n // batch_sz):
    min idx = batch_sz*idx
    max idx = batch_sz*idx+batch_sz
    x_batch,y_batch = X[min_idx;max_idx,:],y[min_idx:max_idx]
    optimizer.zero_grad()
    Loss(model(x_batch),y_batch).backward() 
    optimizer.step()
```

多层全连接网络可以经过单层网络简单的叠加得到，在非线性变换采用 RELU 单元时，模型可通过简单更改得到：

```python
import torch.nn.functional as F
class Net (nn.Module):
  def __init_(self):
     super(Net,self)._init__()
	 self.fc1 = nn.Linear(p,p)
	 self.fc2 = nn.Linear(p,p)
	 self.fc3 = nn.Linear(p,1)
  def forward(self,x):
    x = F.relu(self.fcl(x))
	x = F.relu(self.fc2(x))
	x = self.fc3(x) 
	return x
```


## 2.2 Backpropagation

反向传播算法是目前深度学习框架主要采用的训练求解算法。
详细介绍参考：[http://neuralnetworksanddeeplearning.com/chap2.html](http://neuralnetworksanddeeplearning.com/chap2.html)

* Input $x$ : Set the corresponding activation $a^1$ for the input layer.

* Feedforward : For each $l=2,3,\cdots,L$ compute $z^l=w^l a^{l−1}+b^l$ and $a^l=\sigma(z^l)$

* Output error $\delta^L$ : Compute the vector $\delta^L=\nabla_a C \odot \sigma' (z^L)$

* Backpropagate the error : For each $l=L−1,L−2,\cdots,2$ compute $\delta^1=((w^{l+1})^T \delta^{l+1}) \odot \sigma'(z^l)$

* Output : The gradient of the cost function is given by $\frac{\partial C} {\partial w^l_{jk}}=a^{l−1}_k \delta^l_j$ and $\frac{\partial C} {\partial b^l_j}=\delta^l_j$


We’ll use $w^l_{jk}$ to denote the weight for the connection from the $k^{th}$ neuron in the $(l−1)^{th}$ layer to the $j^{th}$ neuron in the $l^{th}$ layer.

We use $b^l_j$ for the bias of the $j^{th}$ neuron in the $l^{th}$ layer. And we use $a^l_j$ for the activation of the $j^{th}$ neuron in the $l^{th}$ layer.

We use $s\odot t$ to denote the elementwise product of the two vectors. Thus the components of $s\odot t$ are just $(s\odot t)_j=s_j*t_j$.



## 2.3 backward

以 $ y = f(wx+b) $为例，前向和反向传播过程如图。

{% include figure.html  width="488" src="/figures/20210721-DL/bp.jpg" caption="图7：Forward and Backward"%}

在前向过程中，PyTorch会构建计算图，每个节点用Variable表示，边表示由输入节点到输出节点的函数（torch.autograd.Function对象）。
Function对象不仅负责执行前向计算，在反向过程中，每个Function对象会调用.backward()函数计算输出对输入的梯度，然后将梯度传递给下一个Function对象。

PyTorch具有自动求导功能，实现在 torch.autograd 中。
参考 [pytorch-cn notes autograd](https://github.com/awfssv/pytorch-cn/blob/master/docs/notes/autograd.md)

* 变量 requires_grad 标志：如果输入操作需要梯度，它的输出也需要梯度；当所有输入都不需要梯度时，输出也就不需要梯度，后向计算不会在子图中执行。

* 在前向过程中，PyTorch自动构建计算图，自动编码历史信息：
每个变量都有一个.creator属性，它指向把它作为输出的函数。
这是一个由 Function 对象作为节点组成的有向无环图（DAG）的入口点，它们之间的引用就是图的边。
每次执行一个操作时，一个表示它的新 Function 就被实例化，它的 forward() 方法被调用，并且它输出的 Variable 的创建者被设置为这个 Function。
然后，通过跟踪从任何变量到叶节点的路径，可以重建创建数据的操作序列，并自动计算梯度。

* 需要注意的一点是，整个图在每次迭代时都是从头开始重新创建的，这就允许使用任意的Python控制流语句，这样可以在每次迭代时改变图的整体形状和大小。
在启动训练之前不必对所有可能的路径进行编码—— what you run is what you differentiate.

* 可以用python实现新的autograd，详见 [autograd custom_function](https://pytorch.org/tutorials/beginner/examples_autograd/two_layer_net_custom_function.html)



## 2.4 PyTorch 中 backward 的实现

### torch.Tensor.backward

根据函数定义 torch.Tensor.backward，或者 tensor.backward() 其实是调用 torch.autograd.backward。

```python
def backward(self, gradient=None, retain_graph=None, create_graph=False):
    ...
    torch.autograd.backward(self, gradient, retain_graph, create_graph) 
```

### torch.autograd.backward

torch.autograd.backward会执行 run_backward，
在 run_backward 之前，会先调用函数 _make_grads，该函数对 grad_tensors 中的元素进行检查，并将 grad_tensors 重新组织成 tuple(list(torch.Tensor)) 的形式。
_make_grads 后，调用 Variable._execution_engine.run_backward 。

```python
def _make_grads(outputs, grads):
    new_grads = []
    for out, grad in zip(outputs, grads):
        if isinstance(grad, torch.Tensor):
            if not out.shape == grad.shape:
                # raise RuntimeError ...
            new_grads.append(grad)
        elif grad is None:
            if out.requires_grad:
                if out.numel() != 1:
                    # raise RuntimeError ...
            else:
                new_grads.append(None)
        else:
            # raise TypeError ...
    return tuple(new_grads)


def backward(tensors, grad_tensors=None, retain_graph=None, create_graph=False, grad_variables=None):
    ...
    grad_tensors = _make_grads(tensors, grad_tensors)
    if retain_graph is None:
        retain_graph = create_graph

    Variable._execution_engine.run_backward(
        tensors, grad_tensors, retain_graph, create_graph,
        allow_unreachable=True)  # allow_unreachable flag
```

### Variable._execution_engine.run_backward

可以看到，函数 Variable._execution_engine.run_backward 其实就是 torch._C 中的函数 _ImperativeEngine。
torch._C 这个是调用的被编译之后的 C++ 代码。

```python
from torch._C import _ImperativeEngine as ImperativeEngine
Variable._execution_engine = ImperativeEngine()
```

### torch._C._ImperativeEngine

函数 _ImperativeEngine 的实现在文件 torch/csrc/autograd/python_engine.cpp 中。
通过函数 PyModule_AddObject 将 (PyObject *)&THPEngineType 这个 object 加入到模块 module 中，并命名为 _ImperativeEngine。

```c++
bool THPEngine_initModule(PyObject *module)
{
  PyModule_AddObject(module, "_ImperativeEngine", (PyObject *)&THPEngineType);
  set_default_engine_stub(get_python_engine);
  return true;
}
```

### _ImperativeEngine().run_backward()

代码上看，Variable._execution_engine.run_backward() 调用的就是 _ImperativeEngine().run_backward()。
从对象 THPEngineType 的定义看到， _ImperativeEngine().run_backward() 其实是 THPEngine_run_backward。
THPEngine_run_backward 经过处理后，调用 engine.execute()。

```c++
static struct PyMethodDef THPEngine_methods[] = {
  {(char*)"run_backward", (PyCFunction)(void(*)(void))THPEngine_run_backward, METH_VARARGS | METH_KEYWORDS, nullptr},
  {(char*)"queue_callback", (PyCFunction)THPEngine_queue_callback, METH_O, nullptr},
  {(char*)"is_checkpoint_valid", (PyCFunction)THPEngine_is_checkpoint_valid, METH_NOARGS, nullptr},
  {nullptr}
};
```

```c++
// Implementation of torch._C._EngineBase.run_backward
PyObject *THPEngine_run_backward(THPEngine *self, PyObject *args, PyObject *kwargs)
{
    //...	
    variable_list outputs;
    {
        pybind11::gil_scoped_release no_gil;
        outputs = engine.execute(roots, grads, keep_graph, create_graph, output_edges);
    }
}
```


### PythonEngine::execute()

PythonEngine::execute 定义如下:

```c++
struct PythonEngine : public Engine {
  void thread_init(int device) override;
  void thread_on_exception(
      std::shared_ptr<GraphTask>& graph_task,
      const std::shared_ptr<Node>& fn,
      std::exception& e) override;
  variable_list execute(
      const edge_list& roots,
      const variable_list& inputs,
      bool keep_graph,
      bool create_graph,
      const edge_list& outputs = {}) override;

  variable_list execute_with_graph_task(
      std::shared_ptr<GraphTask> graph_task,
      std::shared_ptr<Node> graph_root) override;
  std::unique_ptr<AnomalyMetadata> make_anomaly_metadata() override;
};
```

### Engine::execute()

```c++
auto Engine::execute(const edge_list& roots,
                     const variable_list& inputs,
                     bool keep_graph,
                     bool create_graph,
                     const edge_list& outputs) -> variable_list {
  //...

  return execute_with_graph_task(graph_task, graph_root)->wait();
}
```

### execute_with_graph_task()

```c++
std::shared_ptr<FutureVariableList> Engine::execute_with_graph_task(
    const std::shared_ptr<GraphTask>& graph_task,
    std::shared_ptr<Node> graph_root) {
      //...
      thread_main(graph_task, /* reentrant_thread */ true);
      //...
}
```

### thread_main

```c++
auto Engine::thread_main(
    const std::shared_ptr<GraphTask>& graph_task,
    bool reentrant_thread) -> void {
    //...
      if (task.fn_ && !local_graph_task->has_error_.load()) {
        AutoGradMode grad_mode(local_graph_task->grad_mode_);
        try {
          evaluate_function(local_graph_task, task.fn_.get(), task.inputs_);
        } catch (std::exception& e) {
          thread_on_exception(local_graph_task, task.fn_, e);
        }
      }
    //...
}
```

### evaluate_function

```c++
void Engine::evaluate_function(std::shared_ptr<GraphTask>& graph_task,
                               Node* func, 
                               InputBuffer& inputs){
    //...
     auto outputs = call_function(graph_task, func, inputs);
    //...

```

### call_function

```c++
variable_list call_function(std::shared_ptr<GraphTask>& graph_task,
                            Node* func,
                            InputBuffer& inputBuffer) {
    auto& fn = *func;
    auto inputs = call_pre_hooks(fn, InputBuffer::variables(std::move(inputBuffer)));
    variable_list outputs = fn(std::move(inputs));

    if(has_post_hooks){
        // NOLINTNEXTLINE(bugprone-use-after-move)
        return call_post_hooks(fn, std::move(outputs), inputs);
    }
    return outputs;
}

```

```c++
static variable_list call_pre_hooks(Node& fn, 
                                    variable_list inputs) {
    for (const auto& hook : fn.pre_hooks()) {
        inputs = (*hook)(inputs);
    }
    return inputs;
}

static variable_list call_post_hooks(Node& fn, 
                                     variable_list outputs, 
                                     const variable_list& inputs) {
    for (const auto& hook : fn.post_hooks()) {
        outputs = (*hook)(outputs, inputs);
    }
    return outputs;
}
```

```c++
using Variable = at::Tensor;
using variable_list = std::vector<Variable>;

struct TORCH_API FunctionPreHook {
    virtual ~FunctionPreHook();
    virtual variable_list operator()(const variable_list& grads) = 0;
};

struct TORCH_API FunctionPostHook {
    virtual ~FunctionPostHook();
    virtual variable_list operator()(
        const variable_list& outputs /* grad_inputs */,
        const variable_list& inputs /* grad_outputs */) = 0;
};
```

代码写到这个程度，厉害了！！


## 2.5 optimizer


优化器主要是在模型训练阶段对模型可学习参数进行更新, 在训练过程中先调用 optimizer.zero_grad() 清空梯度，再调用 loss.backward() 反向传播，最后调用 optimizer.step()更新模型参数。

常用优化器有 SGD 及其变形改进版本，如 RMSprop，Adam，ASGD，Adadelta，Adagrad，AdamW，Adamax，SparseAdam，Rprop，LBFGS 等。

### 2.5.1 SGD

有了 $ \partial L/ \partial w$ 和 $ \partial L/ \partial b$ 之后，便可以使用 SGD 进行模型参数更新。


Repeat following steps until converged:
*	Calculate $ \partial L/ \partial w$, $ \partial L/ \partial b$ 
*	Update $ w = w - \eta*\partial L/ \partial w$
*	Update $ b = b - \eta*\partial L/ \partial b$ 


### 2.5.2 优化过程改进

朴素 SGD 在实际使用中，存在诸多问题，一般从三个方面改进优化训练过程。

* 正则化模型改进

为了改进模型性质，可通过对模型结构进行约束，添加正则项得到具有更好统计性质的模型。
如，$L_2$ 正则项 $\|w\|_2$。

模型参数更新从：优化目标函数：$ L $，更新参数：\[ w = w - \eta*\partial L/ \partial w\]

改变为：
优化目标函数：$ L' = L + 0.5* \lambda* |w |_2$, 更新参数：\[ w = w - \eta* \partial L/ \partial w -\eta* \lambda*w.\]


* 动量加速训练过程

为了加速训练过程收敛速度，可通过添加适当的动量来加速收敛。

例如，取动量 $\triangle w = -\partial L/ \partial w$, 权重更新为：

\[ w = w + \triangle w + m \triangle w_{pre}\]


* 训练过程稳定化技巧

例如，BERT中采用LayerNorm，公式如下：

\[\mu = \frac{1}{H}\sum\limits_{i=1}^H x_i\]

\[\sigma = \sqrt{\frac{1}{H}}\sum\limits_{i=1}^H (x_i-\mu)^2\]

\[
LN(x)=g\odot \frac{x-\mu}{\sqrt{\sigma^2+\epsilon}} + b
\tag{1}\label{LN}
\]

其中，$\odot$是element-wise相乘，$g$ 和 $b$ 是可学习的参数。



### 2.5.3 optimizer 实现


PyTorch中，Optimizer 是所有优化器的父类，它主要有如下公共方法:
* add_param_group(param_group): 添加模型可学习参数组；
* step(closure): 进行一次参数更新；
* zero_grad(): 清空上次迭代记录的梯度信息；
* state_dict(): 返回 dict 结构的参数状态；
* load_state_dict(state_dict): 加载 dict 结构的参数状态。




## 2.6 operator 实现

PyTorch 框架解析参考 [ezyang’s blog](http://blog.ezyang.com/2019/05/pytorch-internals/)

在 Forward 中使用的算子在代码中有直接的定义，在 Backward 中使用的算子是在编译过程中自动生成的，依据的定义详见 [derivatives.yaml](https://github.com/pytorch/pytorch/blob/master/tools/autograd/derivatives.yaml)。
把ATen的反向传播API转换成Function，在ATen的正向传播API中加入建图过程。

{% include figure.html  width="488" src="/figures/20210721-DL/operations.jpg" caption="图：PyTorch中 Operations"%}




## 2.7 profiler

torch.autograd.profiler 提供function级别的统计信息，
输出为包含 CPU 时间及占比，调用次数等信息（由于一个 kernel 可能还会调用其他 kernel，因此 Self CPU 指他本身所耗时间（不含其他 kernel 被调用所耗时间））：
```python
import torch
from torchvision.models import resnet18

x = torch.randn((1, 3, 224, 224), requires_grad=True)
model = resnet18()
with torch.autograd.profiler.profile() as prof:
    for _ in range(100):
        y = model(x)
        y = torch.sum(y)
        y.backward()
# NOTE: some columns were removed for brevity
print(prof.key_averages().table(sort_by="self_cpu_time_total"))
```


# 三、理解深度学习

接下来有时间了，再写目前学术社区对深度学习的理论理解。

