---
layout: post
mathjax: true
catalog: true
comments: true
top-tags-list: true
header-img: "img/post-bg-universe.jpg"
header-mask: 0.4
title: 篇章级事件抽取2
subtitle: 基于英文数据集的篇章级事件抽取文章阅读笔记
author: 阚志刚
tags: [英文, 篇章级, 事件抽取]
---


# 一、序言
这一篇主要记录的是基于RAMS和WikiEvents这里两个英文数据集的篇章级事件抽取工作的文章，与中文的经融领域篇章级数据集不同的是，这两个数据集中提供了触发词。

# 二、 TSAR

## 2.1 基本介绍
TSAR出自《[A Two-Stream AMR-enhanced Model for Document-level Event Argument Extraction](https://arxiv.org/pdf/2205.00241.pdf)》，作者来自北京大学和腾讯云小微，看文章格式应该是在投ARR。

## 2.2 文章动机和思路

{% include figure.html src="/figures/2022-06-01-doc_event/TSAR_example.png" caption="图1.1：篇章级事件抽取任务示例。" %}
如图1.1所示，篇章级事件抽取任务存在两个挑战：1）触发词和事件元素之间的**长距离依赖**，即在篇章级的场景中事件元素可能在不同的句子中；2）**干扰文本**，即篇章中存在冗余的候选元素可能会对元素抽取造成干扰。因此本文提出了使用全局和局部两个编码器来从不同的视角对文章信息编码以及交互各候选元素间的信息，然后将这两个视角下的词向量融合之后得到最终的事件元素表示，用于元素分类。由于元素抽取是从文章中抽取跨度，本文为了提高跨度抽取的性能提出了一个辅助边界损失函数（auxiliary boundary loss）与多分类常用的交叉熵一起组成最终的损失函数。

## 2.3 方法

图1.2展示了TSAR的结构。该模型主要由编码器模块、信息交互模块，融合模块以及分类模块组成。 模型从全局和局部两个角度来分别捕捉篇章中的信息，然后进行融合。在抽取事件时，TSAR是先抽取跨度（span）然后再对跨度进行分类。下面介绍一下各个模块。

{% include figure.html src="/figures/2022-06-01-doc_event/TSAR_overview.png" caption="图1.2：TSAR模型的概览。" %}

### 2.3.1 编码器模块

编码器模块包括两种编码器：全局编码器和局部编码器。这两者在网络结构上是一样的，都是“基于Transformer的预训练语言模型”（例如BERT）。全局编码器就是使用原始的transformer，它能看见输入序列中所有的句子，其注意力计算公式如下:
{% include figure.html src="/figures/2022-06-01-doc_event/TSAR_att1.png"%}
局部编码器的设计初衷是让输入序列中的每一个token只能看到它所在的句子和触发词所在的句子，因此本文使用了一个$M$矩阵来将需要遮掩的句子遮住，局部编码器的注意力公式如下：
{% include figure.html src="/figures/2022-06-01-doc_event/TSAR_att2.png"%}
其中矩阵$M$的定义如下：
{% include figure.html src="/figures/2022-06-01-doc_event/TSAR_M.png"%}
经过编码器之后得到文章的全局编码和局部编码：
{% include figure.html src="/figures/2022-06-01-doc_event/TSAR_encode.png"%}
其中，$ \|D\| $ 表示文章中单词的数量。

### 2.3.2 信息交互模块

在这个模块中，TSAR先是基于AMR分别构建了全局语义图和局部语义图。局部语义图是分别为每个句子构建一个AMR图，全局语义图是在局部语义图的基础上将所有句子AMR图的根节点连接起来。
信息交互模块遵循“组合（composition），交互（interaction）和分解（decomposition）”范式。以局部语义图为例，在组合阶段，本文首先是对跨度中的每一个token的编码求均值得到节点的编码表示：
{% include figure.html src="/figures/2022-06-01-doc_event/TSAR_composed.png"%}
然后使用$L$层GCN对图进行卷积，达到信息交互的目的：
{% include figure.html src="/figures/2022-06-01-doc_event/TSAR_gcn.png"%}
节点表示是每一层GCN的输出拼接后乘上可学习的矩阵$W_1$: $ h_u=W_1[h_n^0; h_n^1; ...; h_n^L] $.
接着是分解操作。即使用token原始的编码$Z_i^L$加上来自交互后图的信息：
文首先是对跨度中的每一个token的编码求均值得到节点的编码表示：
{% include figure.html src="/figures/2022-06-01-doc_event/TSAR_node_embedding.png"%}
基于全局图的信息交互与上述过程类似。


### 2.3.3 信息融合模块

这部分的目的是融合全局信息编码和局部信息编码。做法上就是使用一个门控向量来控制最终的token表示中全局信息和局部信息的比例：
{% include figure.html src="/figures/2022-06-01-doc_event/TSAR_gate.png"%}
候选跨度的表示为：
{% include figure.html src="/figures/2022-06-01-doc_event/TSAR_span_embedding.png"%}
{% include figure.html src="/figures/2022-06-01-doc_event/TSAR_head_end.png"%}
文章在这里还介绍了一种便捷损失函数。具体而言就是对$\tilde{h}_{start}$和 $\tilde{h}_{end}$时候是golden span的概率进行计算然后基于交叉熵求损失函数：
{% include figure.html src="/figures/2022-06-01-doc_event/TSAR_position_1.png"%}
{% include figure.html src="/figures/2022-06-01-doc_event/TSAR_position_2.png"%}

### 2.3.4 分类模块

在对候选跨度进行分类时，首先使用各种信息拼接形成最终的span表示：
{% include figure.html src="/figures/2022-06-01-doc_event/TSAR_span_final.png"%}
分类网络是前馈神经网络，分类损失函数是交叉熵。最终的损失函数是分类损失函数和跨度算是函数的加权和。


# 三、 BART-Gen

## 3.1 基本介绍

BART-Gen这个工作出自NAACL2021的文章《[Document-Level Event Argument Extraction by Conditional Generation](https://aclanthology.org/2021.naacl-main.69.pdf)》，作者是来自伊利诺伊大学香槟分校的团队。

## 3.2 动机

{% include figure.html src="/figures/2022-06-01-doc_event/BART_example.png" caption="图3.1：篇章级事件抽取示例。" %}


如图3.1所示：在篇章级事件任务中，1）事件元素往往会分布于不同的句子中； 2）事件提及中出现的代词形式的元素需要去其他句子中寻找先行语。因此模型需要有跨句子抽取事件元素的能力和识别代词先行语的能力。本文除了基于prompt范式还公开了一个篇章级的事件抽取数据集：WikiEvents。


## 3.3 方法

{% include figure.html src="/figures/2022-06-01-doc_event/BART_overview.png" caption="图3.2：BART-Gen模型工作原理示例。" %}

这篇文章的在进行篇章级的事件元素抽取时的思想很简单，就是基于prompt范式，将篇章级事件抽取任务转换成一种完型填空任务，然后通过BART填补完形填空题干中的空缺。值得注意的是这里的提示模板是事件类型特定（type-specific）的，也就是说为每一个事件类型都设计一个涵盖了所有事件元素的提示模板。实际上这里的方法没有体现篇章级事件抽取的特征，这个方法可以直接用到句子级的事件抽取工作上。
本文还提出了一个基于关键词的触发词抽取方法，我认为它跟篇章级的抽取工作也没有太密切的联系，所以在这里不说了。


# 四、CUP

## 4.1 基本介绍

CUP来自文章《[CUP: Curriculum Learning based Prompt Tuning for Implicit Event Argument Extraction](https://arxiv.org/pdf/2205.00498.pdf)》，是来自华东师范和复旦大学的一篇工作，目前还放在arxiv上。

## 4.2 方法

{% include figure.html src="/figures/2022-06-01-doc_event/CUP_training.png" caption="图4.1：CUP模型训练过程。" %}

这个工作也是一个基于prompt范式的工作，prompt的构建与BART-Gen相似。我认为这篇文章的亮点有两个，一是使用课程式学习的思想来逐步完成篇章级事件抽取任务，二是利用结构化的图信息来构建prompt。模型的训练过程如图4.1所示，本文将模型的训练分成四个阶段，第一个阶段是对触发词所在的句子进行事件元素抽取（假设触发词已知）；第二阶段是将与触发词所在的句子有相同实体的句子考虑进来，此阶段的prompt中第一阶段的结果是填好的；第三阶段则是将整片文章作为输入文本，此时还加入了图结构的prompt；第四阶段则是在第三阶段的基础上去掉了所有在前面阶段知道的答案，让模型直接预测所有的事件元素。这种课程式学习的好处是让那个模型可以在不同的学习阶段专注学习与触发词距离不同的事件元素的特征。


本文在三四阶段使用了基于结构化子图的prompt，主要原理是给定触发词和目标事件元素这两个节点，按照预定的规则寻找一个最小子图，让模型根据自途中的其他节点来预测目标事件元素节点。具体做法是将这些AMR子图利用AMR解析和生成技术生成文本序列，然后结合原文输入到预训练语言模型中，让模型预测目标元素。


{% include figure.html src="/figures/2022-06-01-doc_event/CUP_inference.png" caption="图4.2：CUP模型的推理过程。" %}

这个工作的推理过程与训练过程不太一样，推理过程的流程如图4.2所示。由于在推理过程中不知道哪些句子中蕴含事件元素，因此在分阶段推理的时候，本文的做法是先对触发词所在的句子进行事件元素抽取。再根据这个句子中的事件元素，找出包含这些事件元素的其他句子，进行下一阶段的事件元素抽取，最后一个阶段是对整片文章做事件抽取。


# 五、 DocMRC

## 5.1 基本情况

这个工作出自文章《[Machine Reading Comprehension as Data Augmentation: A Case Study on Implicit Event Argument Extraction](https://aclanthology.org/2021.emnlp-main.214.pdf)》。文章发表在EMNLP2021，是北京交通大学的一个工作。

## 5.2

{% include figure.html src="/figures/2022-06-01-doc_event/DocMRC.png" caption="图5.1：DocMRC的两种用法。" %}

这篇文章理解起来比较简单，文中提出来的用阅读理解来解决篇章级事件抽取的方法与句子级的基于MRC的方法没什么区别，不同之处在于本文提出了两种用法。
如图5.1左边部分所示，第一种用法迁移学习。即首先在其他任务上做训练，然后在篇章级事件抽取任务上做微调。第二种用法是将利用MRC来自动标注数据，作为篇章级事件抽取模型的训练数据。


# 六、 总结

上面几个工作是基于英文数据集的几个有代表性的工作，我认为可以分成三类：基于文章图+分类的工作、基于prompt范式的工作以及基于MRC的工作。其中基于图的工作是基于中文的篇章级事件抽取工作常用的做法，不过我觉得如何利用好图的信息还是一个进一步讨论的问题。基于prompt的方法是最近比价火热的，这种方法基于transformer架构的特点可以无视距离因素，对远距离的事件元素进行较好的抽取。不过感觉基于prompt范式没有什么可以继续做的工作了。基于MRC的工作主要面向数据稀疏甚至零样本的场景，实际上这里提到的迁移学习的模式也可以用基于prompt的方法来实现。