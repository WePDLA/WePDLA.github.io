---
layout: post
mathjax: true
catalog: true
comments: true
top-tags-list: true
header-img: "img/post-bg-universe.jpg"
header-mask: 0.4
title: 篇章级事件抽取2
subtitle: 基于英文数据集的篇章级事件抽取文章阅读笔记
author: 阚志刚
tags: [英文, 篇章级, 事件抽取]
---


# 一、序言
这一篇主要记录的是基于RAMS和WikiEvents这里两个英文数据集的篇章级事件抽取工作的文章，与中文的经融领域篇章级数据集不同的是，这两个数据集中提供了触发词。

# 二、 TSAR

## 2.1 基本介绍
TSAR出自《[A Two-Stream AMR-enhanced Model for Document-level Event Argument Extra](https://arxiv.org/pdf/2205.00241.pdf)》，作者来自北京大学和腾讯云小微，看文章格式应该是在投ARR。

## 2.2 文章动机和思路

{% include figure.html src="/figures/2022-06-01-doc_event/TSAR_example.png" caption="图1.1：篇章级事件抽取任务示例。"%}
如图1.1所示，篇章级事件抽取任务存在两个挑战：1）触发词和事件元素之间的**长距离依赖**，即在篇章级的场景中事件元素可能恩不在不同的句子中；2）**干扰文本**，即篇章中存在更多的候选元素可能会对元素抽取造成干扰。因此本文提出了使用全局和局部两个编码器来从不同的视角对文章信息编码以及交互各候选元素间的信息，然后将这两个视角下的词向量融合之后得到最终的事件元素表示，用于元素分类。由于元素抽取是从文章中抽取跨度，本文为了提高跨度抽取的性能提出了一个辅助边界损失函数（auxiliary boundary loss）与多分类常用的交叉熵一起组成最终的损失函数。

## 2.3 方法

图1.2展示了TSAR的结构。该模型主要由编码器模块、信息交互模块，融合模块以及分类模块组成。 模型从全局和局部两个角度来分别捕捉篇章中的信息，然后进行融合。在抽取事件时，TSAR是先抽取跨度（span）然后再对跨度进行分类。下面介绍一下各个模块。

{% include figure.html src="/figures/2022-06-01-doc_event/TSAR_overview.png" caption="图1.2：TSAR模型的概览。"%}

### 2.3.1 编码器模块

编码器模块包括两种编码器：全局编码器和局部编码器。这两者在网络结构上是一样的，都是“基于Transformer的预训练语言模型”（例如BERT）。全局编码器就是使用原始的transformer，它能看见输入序列中所有的句子，其注意力计算公式如下:
{% include figure.html src="/figures/2022-06-01-doc_event/TSAR_att1.png"}
局部编码器的设计初衷是让输入序列中的每一个token只能看到它所在的句子和触发词所在的句子，因此本文使用了一个$M$矩阵来将需要遮掩的句子遮住，局部编码器的注意力公式如下：
{% include figure.html src="/figures/2022-06-01-doc_event/TSAR_att2.png"}
其中矩阵$M$的定义如下：
{% include figure.html src="/figures/2022-06-01-doc_event/TSAR_M.png"}
经过编码器之后得到文章的全局编码和局部编码：
{% include figure.html src="/figures/2022-06-01-doc_event/TSAR_encode.png"}
$|D|$表示文章中单词的数量。

### 2.3.2 信息交互模块

在这个模块中，TSAR先是基于AMR分别构建了全局语义图和局部语义图。局部语义图是分别为每个句子构建一个AMR图，全局语义图是在局部语义图的基础上将所有句子AMR图的根节点连接起来。
信息交互模块遵循“组合（composition），交互（interaction）和分解（decomposition）”范式。以局部语义图为例，在组合阶段，本文首先是对跨度中的每一个token的编码求均值得到节点的编码表示：
{% include figure.html src="/figures/2022-06-01-doc_event/TSAR_composed.png"}
然后使用$L$层GCN对图进行卷积，达到信息交互的目的：
{% include figure.html src="/figures/2022-06-01-doc_event/TSAR_gcn.png"}
节点表示是每一层GCN的输出拼接后乘上可学习的矩阵$W_1$: $ h_u=W_1[h_n^0; h_n^1; ...; h_n^L] $.
接着是分解操作。即使用token原始的编码$Z_i^L$加上来自交互后图的信息：
文首先是对跨度中的每一个token的编码求均值得到节点的编码表示：
{% include figure.html src="/figures/2022-06-01-doc_event/TSAR_node_embedding.png"}
基于全局图的信息交互与上述过程类似。


### 2.3.3 信息融合模块

这部分的目的是融合全局信息编码和局部信息编码。做法上就是使用一个门控向量来控制最终的token表示中全局信息和局部信息的比例：
{% include figure.html src="/figures/2022-06-01-doc_event/TSAR_gate.png"}
候选跨度的表示为：
{% include figure.html src="/figures/2022-06-01-doc_event/TSAR_span_embedding.png"}
{% include figure.html src="/figures/2022-06-01-doc_event/TSAR_head_end.png"}
文章在这里还介绍了一种便捷损失函数。具体而言就是对$\tilde{h}_{start}$和$\tilde{h}_{end}$时候是golden span的概率进行计算然后基于交叉熵求损失函数：
{% include figure.html src="/figures/2022-06-01-doc_event/TSAR_position_1.png"}
{% include figure.html src="/figures/2022-06-01-doc_event/TSAR_position_2.png"}

### 2.3.4 分类模块

在对候选跨度进行分类时，首先使用各种信息拼接形成最终的span表示：
{% include figure.html src="/figures/2022-06-01-doc_event/TSAR_span_final.png"}
分类网络是前馈神经网络，分类损失函数是交叉熵。最终的损失函数是分类损失函数和跨度算是函数的加权和。


