<<<<<<< HEAD
---
layout: post
title: 预训练语言模型
comments: True
author: gaoIf
---

作者：高翊夫
=======
>>>>>>> ee0758c7cf34333963091fa9a5e1845eb8f76ba5
#### 一、ELMO（feature-based）
 论文网址：[Deep contextualized word representations. NAACL 2018.]( https://arxiv.org/abs/1802.05365)
##### 1.  模型
ELMo不会为每个单词使用固定的嵌入，而是会在为每个单词分配嵌入之前先查看整个句子。 它使用在特定任务上受过训练的双向LSTM，能够创建这些嵌入。
实际上，ELMo向前走了一步，并训练了双向LSTM，因此其语言模型不仅具有下一个单词的含义，而且还具有前一个单词的含义。[(ELMO、BERT博客)](http://jalammar.github.io/illustrated-bert/)
<<<<<<< HEAD
![9fb70f753593f082d262a50db9a3a2e3](/figures/预训练模型.resources/截屏2020-03-27下午1.36.10.png)
ELMo通过以某种方式将隐藏状态（和初始嵌入）分组在一起（级联后进行加权求和）来进行上下文化嵌入。
![9afbc3dbb05359ea730e705162da5744](/figures/预训练模型.resources/截屏2020-03-27下午1.37.58.png)
##### 2.  序列标注任务（sequence tagging）[(斯坦福2019第13讲笔记)](https://looperxx.github.io/CS224n-2019-13-Modeling%20contexts%20of%20use%20Contextual%20Representations%20and%20Pretraining/)
ELMo为NLP的培训提供了重要的一步。 ELMo LSTM将以我们数据集的语言在海量数据集上进行训练，然后我们可以将其用作其他需要处理语言的模型的组成部分
![035f70bd01404ae31e282614875c00c9](/figures/预训练模型.resources/截屏2020-03-27下午1.51.53.png)
=======
![9fb70f753593f082d262a50db9a3a2e3](预训练模型.resources/截屏2020-03-27下午1.36.10.png)
ELMo通过以某种方式将隐藏状态（和初始嵌入）分组在一起（级联后进行加权求和）来进行上下文化嵌入。
![9afbc3dbb05359ea730e705162da5744](预训练模型.resources/截屏2020-03-27下午1.37.58.png)
##### 2.  序列标注任务（sequence tagging）[(斯坦福2019第13讲笔记)](https://looperxx.github.io/CS224n-2019-13-Modeling%20contexts%20of%20use%20Contextual%20Representations%20and%20Pretraining/)
ELMo为NLP的培训提供了重要的一步。 ELMo LSTM将以我们数据集的语言在海量数据集上进行训练，然后我们可以将其用作其他需要处理语言的模型的组成部分
![035f70bd01404ae31e282614875c00c9](预训练模型.resources/截屏2020-03-27下午1.51.53.png)
>>>>>>> ee0758c7cf34333963091fa9a5e1845eb8f76ba5
* Char CNN / RNN + Token Embedding 作为 bi-LSTM 的输入
* 得到的 hidden states 与 Pre-trained bi-LM（冻结的） 的 hidden states 连接起来输入到第二层的 bi-LSTM 中
##### 3.  总结

* **ULM-FIT将关注在NLP中的迁移学习**：ULM-FiT引入了一些方法，可以有效地利用模型在预训练期间学到的很多知识-不仅是嵌入，而且是上下文化的嵌入。 ULM-FiT引入了一种语言模型和一种过程，可以针对各种任务有效地微调该语言模型。NLP终于有可能像Computer Vision一样进行迁移学习。
* ELMO的深层Bi-LSTM模型的双向本质上不是并行的，因为RNN是语言模型的网络，顺序预测下一个单词。
* RNN中的变种LSTM和GRU处理长期依赖关系较差，即使有attention机制。
#### 二、Transformer
论文网址：[Attention is all you need.](https://arxiv.org/pdf/1706.03762.pdf)
博客地址[The Illustrated Transformer](https://jalammar.github.io/illustrated-transformer/)
##### 1. 模型概览
transformer做机器翻译任务时的模型概况：
<<<<<<< HEAD
![df47adf2afc89245701d3ebe24323a50](/figures/预训练模型.resources/截屏2020-03-27下午3.13.22.png)
每个encoder和decoder的内部结构如图：
![48f7eb3f8889cd078880bc98f42370c2](/figures/预训练模型.resources/截屏2020-03-27下午3.21.24.png)
=======
![df47adf2afc89245701d3ebe24323a50](预训练模型.resources/截屏2020-03-27下午3.13.22.png)
每个encoder和decoder的内部结构如图：
![48f7eb3f8889cd078880bc98f42370c2](预训练模型.resources/截屏2020-03-27下午3.21.24.png)
>>>>>>> ee0758c7cf34333963091fa9a5e1845eb8f76ba5

* 编码器的输入首先经过一个自我注意层，该层可以帮助编码器在对特定单词进行编码时查看输入句子中的其他单词。 
* 自我注意层的输出被馈送到前馈神经网络。 完全相同的前馈网络独立应用于每个位置。
* 解码器具有这两层，但在它们之间是一个关注层，可以帮助解码器将注意力集中在输入语句的相关部分上（类似于seq2seq模型中的attention）。
##### 2. 模型细节
##### encoder
首先每个单词嵌入成大小为512维的word embedding，然后通过将这些向量传递到“自我注意”层，然后传递到前馈神经网络，然后将输出向上发送到下一个编码器来处理此列表。
<<<<<<< HEAD
![c00ccbed1cbd8093b2700e720b8b35ef](/figures/预训练模型.resources/截屏2020-03-27下午4.09.23.png)
=======
![c00ccbed1cbd8093b2700e720b8b35ef](预训练模型.resources/截屏2020-03-27下午4.09.23.png)
>>>>>>> ee0758c7cf34333963091fa9a5e1845eb8f76ba5
###### self-attention层
Attention函数的本质可以被描述为一个查询（query）到一系列（键key-值value）对的映射。在计算attention时主要分为三步：

* 第一步是将query和每个key进行相似度计算得到权重，常用的相似度函数有点积，拼接，感知机等；
* 第二步一般是使用一个softmax函数对这些权重进行归一化；
* 最后将权重和相应的键值value进行加权求和得到最后的attention。目前在NLP研究中，key和value常常都是同一个，即key=value。
细节如下图：
<<<<<<< HEAD
![ca3a609427702dbce1d355e0d143591e](/figures/预训练模型.resources/截屏2020-03-28下午12.07.52.png)
三种向量分别是隐藏矩阵与三种权重矩阵相乘得到的：
![a905a858c19b79320e20bf44421b1edc](/figures/预训练模型.resources/6A86C3F8-E592-4DA2-92E1-68F50100ACBB.png)
上述过程用矩阵的方式计算如下：
![cd82cf042a8ef0ccfbc4a3d2990299db](/figures/预训练模型.resources/8F5F6213-C00E-4308-9D97-1691E1882BB2.png)
这里word embedding为512维，查询、键值等向量为64维（因为后面多头操作是8个）dk值为64，可以生成更稳定的梯度。
###### Multi-head操作
![ee00545760c1d62110d5cc4266878d81](/figures/预训练模型.resources/1D748B11-550B-4541-AFA9-CD1E46D92B0C.png)
将多头得到的向量拼接在一起，然后乘以矩阵回复到单头的维度
![e7100050e80cc136c33a462a2a5c4c33](/figures/预训练模型.resources/0FA759BC-08EB-4A87-8CD5-0CE1F9CC4FDC.png)
全部操作流程如下图所示：
![99033c9d4d5158d1eaacfc4756529082](/figures/预训练模型.resources/截屏2020-03-28下午1.07.11.png)
###### 正则化以及残差链接
![4430696ff8552075e0f0ff09cad2e22e](/figures/预训练模型.resources/2EE08CB8-D66F-4BDB-8F11-6C5AF2D205D2.png)
##### decoder
decoder与encoder类似 多了一层attention，如下图所示
![c08a5b1fae18edafdaa8bfb9b08547e9](/figures/预训练模型.resources/34E2C1FD-533D-4C4F-9C9C-AA68B2BC7A72.png)
顶部编码器的输出转换为注意向量K和V的集合。每个解码器将在其“编码器-解码器注意”层中使用它们，这有助于解码器将注意力集中在输入序列中的适当位置，类似sequence to sequence的attention，由编码器的输入一个句子多个单词向量转化成在解码器的位置每次输入前一个单词的向量，其他一致
![ff7cec5ce7a8aaabe070ba5f0bc69b00](/figures/预训练模型.resources/8BC11502-BAF2-4E97-93AB-83E05B4D7263.png)
=======
![ca3a609427702dbce1d355e0d143591e](预训练模型.resources/截屏2020-03-28下午12.07.52.png)
三种向量分别是隐藏矩阵与三种权重矩阵相乘得到的：
![a905a858c19b79320e20bf44421b1edc](预训练模型.resources/6A86C3F8-E592-4DA2-92E1-68F50100ACBB.png)
上述过程用矩阵的方式计算如下：
![cd82cf042a8ef0ccfbc4a3d2990299db](预训练模型.resources/8F5F6213-C00E-4308-9D97-1691E1882BB2.png)
这里word embedding为512维，查询、键值等向量为64维（因为后面多头操作是8个）dk值为64，可以生成更稳定的梯度。
###### Multi-head操作
![ee00545760c1d62110d5cc4266878d81](预训练模型.resources/1D748B11-550B-4541-AFA9-CD1E46D92B0C.png)
将多头得到的向量拼接在一起，然后乘以矩阵回复到单头的维度
![e7100050e80cc136c33a462a2a5c4c33](预训练模型.resources/0FA759BC-08EB-4A87-8CD5-0CE1F9CC4FDC.png)
全部操作流程如下图所示：
![99033c9d4d5158d1eaacfc4756529082](预训练模型.resources/截屏2020-03-28下午1.07.11.png)
###### 正则化以及残差链接
![4430696ff8552075e0f0ff09cad2e22e](预训练模型.resources/2EE08CB8-D66F-4BDB-8F11-6C5AF2D205D2.png)
##### decoder
decoder与encoder类似 多了一层attention，如下图所示
![c08a5b1fae18edafdaa8bfb9b08547e9](预训练模型.resources/34E2C1FD-533D-4C4F-9C9C-AA68B2BC7A72.png)
顶部编码器的输出转换为注意向量K和V的集合。每个解码器将在其“编码器-解码器注意”层中使用它们，这有助于解码器将注意力集中在输入序列中的适当位置，类似sequence to sequence的attention，由编码器的输入一个句子多个单词向量转化成在解码器的位置每次输入前一个单词的向量，其他一致
![ff7cec5ce7a8aaabe070ba5f0bc69b00](预训练模型.resources/8BC11502-BAF2-4E97-93AB-83E05B4D7263.png)
>>>>>>> ee0758c7cf34333963091fa9a5e1845eb8f76ba5

#### 三、BERT（fine-tuning）
[博客地址](http://jalammar.github.io/illustrated-bert/)
##### 1. 模型概览
BERT模型使用transformer的encoder部件进行双向连接，模型结构图如下：
<<<<<<< HEAD
![5ce9cd0c3cefd830c89f80e566e3ac02](/figures/预训练模型.resources/EBA8728E-147C-4CCF-91BE-B7FAA591EC7A.png)
BERT官方给出两种结构：
![657a34af3e9e73474b02f67259ebdc74](/figures/预训练模型.resources/D18BC5E4-70CD-4506-8413-B5D93997B542.png)
=======
![5ce9cd0c3cefd830c89f80e566e3ac02](预训练模型.resources/EBA8728E-147C-4CCF-91BE-B7FAA591EC7A.png)
BERT官方给出两种结构：
![657a34af3e9e73474b02f67259ebdc74](预训练模型.resources/D18BC5E4-70CD-4506-8413-B5D93997B542.png)
>>>>>>> ee0758c7cf34333963091fa9a5e1845eb8f76ba5

* BERT-Base：12层encoder，768隐藏单元，12-head attention
* BERT-Large：24层encoder，1024隐藏单元，16-head attention
* transformer：6层encoder，512隐藏单元，8-head attention

##### 2. 预训练任务
BERT实际上是一个语言模型。语言模型通常采用大规模、与特定NLP任务无关的文本语料进行训练，其目标是学习语言本身应该是什么样的，这就好比我们学习语文、英语等语言课程时，都需要学习如何选择并组合我们已经掌握的词汇来生成一篇通顺的文本。回到BERT模型上，其预训练过程就是逐渐调整模型参数，使得模型输出的文本语义表示能够刻画语言的本质，便于后续针对具体NLP任务作微调。为了达到这个目的，BERT文章作者提出了两个预训练任务：Masked LM和Next Sentence Prediction。
###### 2.1 Masked LM
Masked LM的任务描述为：给定一句话，随机抹去这句话中的一个或几个词，要求根据剩余词汇预测被抹去的几个词分别是什么，如下图所示。
<<<<<<< HEAD
![3e3c4d5c8c187acd5d60a087d2ef1f68](/figures/预训练模型.resources/1CF3436D-283E-462A-8037-21C386C13B7E.png)
这不就是我们高中英语常做的完形填空Cloze task么！所以说，BERT模型的预训练过程其实就是在模仿我们学语言的过程。具体来说，文章作者在一句话中随机选择15%的词汇用于预测。对于在原句中被抹去的词汇，80%情况下采用一个特殊符号[MASK]替换，10%情况下采用一个任意词替换，剩余10%情况下保持原词汇不变。这么做的主要原因是：在后续微调任务中语句中并不会出现[MASK]标记，而且这么做的另一个好处是：预测一个词汇时，模型并不知道输入对应位置的词汇是否为正确的词汇（10%概率），这就迫使模型更多地依赖于上下文信息去预测词汇，并且赋予了模型一定的纠错能力。
###### 2.2 NextSentence Prediction
Next Sentence Prediction的任务描述为：给定一篇文章中的两句话，判断第二句话在文本中是否紧跟在第一句话之后，如下图所示。
![9e0f0976204c081d17d8b90f9f9d6924](/figures/预训练模型.resources/1A05570E-AD9C-421E-A09D-7DC30ADA35EC.png)
![b690aa00488a88f32ed6f2297e6ef714](/figures/预训练模型.resources/C65F6F38-245D-4708-9834-B2B3674AD71E.png)
=======
![3e3c4d5c8c187acd5d60a087d2ef1f68](预训练模型.resources/1CF3436D-283E-462A-8037-21C386C13B7E.png)
这不就是我们高中英语常做的完形填空Cloze task么！所以说，BERT模型的预训练过程其实就是在模仿我们学语言的过程。具体来说，文章作者在一句话中随机选择15%的词汇用于预测。对于在原句中被抹去的词汇，80%情况下采用一个特殊符号[MASK]替换，10%情况下采用一个任意词替换，剩余10%情况下保持原词汇不变。这么做的主要原因是：在后续微调任务中语句中并不会出现[MASK]标记，而且这么做的另一个好处是：预测一个词汇时，模型并不知道输入对应位置的词汇是否为正确的词汇（10%概率），这就迫使模型更多地依赖于上下文信息去预测词汇，并且赋予了模型一定的纠错能力。
###### 2.2 NextSentence Prediction
Next Sentence Prediction的任务描述为：给定一篇文章中的两句话，判断第二句话在文本中是否紧跟在第一句话之后，如下图所示。
![9e0f0976204c081d17d8b90f9f9d6924](预训练模型.resources/1A05570E-AD9C-421E-A09D-7DC30ADA35EC.png)
![b690aa00488a88f32ed6f2297e6ef714](预训练模型.resources/C65F6F38-245D-4708-9834-B2B3674AD71E.png)
>>>>>>> ee0758c7cf34333963091fa9a5e1845eb8f76ba5
当年大学考英语四六级的时候，大家应该都做过段落重排序，即：将一篇文章的各段打乱，让我们通过重新排序把原文还原出来，这其实需要我们对全文大意有充分、准确的理解。Next Sentence Prediction任务实际上就是段落重排序的简化版：只考虑两句话，判断是否是一篇文章中的前后句。在实际预训练过程中，文章作者从文本语料库中随机选择50%正确语句对和50%错误语句对进行训练，与Masked LM任务相结合，让模型能够更准确地刻画语句乃至篇章层面的语义信息。

BERT模型通过对Masked LM任务和Next Sentence Prediction任务进行联合训练，使模型输出的每个字/词的向量表示都能尽可能全面、准确地刻画输入文本（单句或语句对）的整体信息，为后续的微调任务提供更好的模型参数初始值。
##### 3. 几个微调（fine-tuning）的训练任务
<<<<<<< HEAD
![228a3cde8d3901be394bd81c052eb3dd](/figures/预训练模型.resources/2F668D3A-878D-4553-9BAD-B2C44BFE7BBC.png)
=======
![228a3cde8d3901be394bd81c052eb3dd](预训练模型.resources/2F668D3A-878D-4553-9BAD-B2C44BFE7BBC.png)
>>>>>>> ee0758c7cf34333963091fa9a5e1845eb8f76ba5
Bert模型以及tensorflow1.0实现：[github-google/bert](https://github.com/google-research/bert)
pytorch及tensorflow2.0实现： [PyTorch implementation of BERT](https://github.com/huggingface/transformers)