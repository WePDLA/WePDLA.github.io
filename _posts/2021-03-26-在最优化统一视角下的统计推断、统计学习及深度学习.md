---
layout: post
title: 在最优化统一视角下的统计推断、统计学习及深度学习
comments: True
author: 乔林波、刘睿
---

作者： 乔林波、刘睿


在最优化的视角下，机器学习的多个分支可以有效统一，比如：统计推断、统计学习与深度学习。


#  一、统计推断

统计推断中核心任务为：假设有观测的数据集为 $$\mathcal O =\{x_i\}, i=[1, 2, \cdots, n]$$，需要由观测数据推断出最优模型参数$$\mathcal P=\{\mu\}$$。
根据假设的不同，统计推断可以分为两个学派，频率学派和贝叶斯学派。
频率学派的观点是仅使用数据的条件分布，不承认先验概率。
贝叶斯学派的观点是综合利用先验信息和样本信息。
可以认为，频率学派观点是贝叶斯学派观点的一个特例。

## 1.1 统计推断一般范式

对于频率学派，认为$$\mathcal P=\{\mu\}$$是某个确定的实数，使用最大似然或最大对数似然进行统计推断。
对于贝叶斯学派，认为 $$\mathcal P$$ 符合某一先验分布，结合先验分布，使用最大后验估计或最大对数后验估计进行统计推断，其中，通过不同的先验分布可以推导出不同的模型。

综合频率学派和贝叶斯学派，假设随机变量$$X$$服从高斯分布，方差$$\delta^2=1$$：

$$X \sim P(x|\mathcal P) = \frac{1}{\sqrt{2\pi}} e^{-\frac{(x-\mu)^2}{2}},$$

推断最优模型参数也即最大化如下问题：

$$ argmax_{\mu \in (-\infty, +\infty)} \log P(\mathcal O | \mathcal P) P(\mathcal P).\tag{1}$$

具体说来，推断过程如下：

## 1.2 频率学派统计推断

对于频率学派，认为$$\mathcal P=\{\mu\}$$是某个确定的实数，直接有：
$$
\begin{array}{cl}
argmax_{\mu \in (-\infty, +\infty)} f(\mu) &= argmax_{\mu \in (-\infty, +\infty)} \log P(\mathcal O | \mathcal P)\\
  & = argmax_{\mu \in (-\infty, +\infty)} \log \left(  \prod_{i=1}^n P( x_i \in \mathcal O | \mathcal P)\right) \\
  & = argmax_{\mu \in (-\infty, +\infty)} \log  \left( (\frac{1}{\sqrt{2\pi}})^n e^{-\frac{\sum_{i=1}^n(x_i -\mu)^2 }{2}}\right)  \\
  & = -argmax_{\mu \in (-\infty, +\infty)} (\frac{1}{2} \sum_{i=1}^n (x_i -\mu)^2 + n\log \sqrt{2\pi})  
\tag{2} \label{prob-fre}
\end{array}
$$

## 1.3 贝叶斯学派统计推断（高斯分布）

对于贝叶斯学派，认为 $$\mathcal P$$ 符合某一先验分布，在假设为高斯分布$$\mathcal P=\{\mu\}, \mu \sim \mathcal N(0, \delta_0^2)$$时，有：
$$
\begin{array}{cl}
argmax_{\mu \in (-\infty, +\infty)} f(\mu) &= argmax_{\mu \in (-\infty, +\infty)} \log P(\mathcal O | \mathcal P) P(\mathcal P)\\
& = argmax_{\mu \in (-\infty, +\infty)} \log \left( \prod_{i=1}^n P( x_i \in \mathcal O | \mathcal P) P(\mathcal P)\right)  \\
& = argmax_{\mu \in (-\infty, +\infty)}  \log \left( \left( (\frac{1}{\sqrt{2\pi}})^n e^{-\frac{\sum_{i=1}^n(x_i -\mu)^2 }{2}}\right) (\frac{1}{\sqrt{2 \pi} \delta_0}) e^{-\frac{\mu^2}{2\delta_0^2}} \right) \\
& = -argmax_{\mu \in (-\infty, +\infty)} (\frac{1}{2} \sum_{i=1}^n (x_i -\mu)^2 + n\log \sqrt{2\pi} + \frac{1}{2 \delta_0^2}\mu^2 + \log{\sqrt{2 \pi}} \delta_0)  
\tag{3}\label{prob-b-norm}
\end{array}
$$

## 1.4 贝叶斯学派统计推断（拉普拉斯分布）

对于贝叶斯学派，认为 $$\mathcal P$$ 符合某一先验分布，在假设为拉普拉斯分布$$\mathcal P=\{\mu\}, \mu \sim \mathcal L(0, \delta_0^2)$$时，有：
 $$
\begin{array}{cl}
argmax_{\mu \in (-\infty, +\infty)} f(\mu) &= argmax_{\mu \in (-\infty, +\infty)} \log P(\mathcal O | \mathcal P) P(\mathcal P)\\
& = argmax_{\mu \in (-\infty, +\infty)} \log \left( \prod_{i=1}^n P( x_i \in \mathcal O | \mathcal P) P(\mathcal P)\right)  \\
& = argmax_{\mu \in (-\infty, +\infty)}  \log \left( \left( (\frac{1}{\sqrt{2\pi}})^n e^{-\frac{\sum_{i=1}^n(x_i -\mu)^2 }{2}}\right) (\frac{1}{2 \delta_0^2}) e^{-\frac{|\mu|}{\delta_0^2}} \right)  \\
& = -argmax_{\mu \in (-\infty, +\infty)} (\frac{1}{2} \sum_{i=1}^n (x_i -\mu)^2 + n\log \sqrt{2\pi} + \frac{1}{\delta_0^2} |\mu| + \log 2 \delta_0^2)
\tag{4}\label{prob-b-lap}
\end{array}
$$

## 1.5 贝叶斯学派统计推断（拉普拉斯和高斯混合分布）

对于贝叶斯学派，认为 $$\mathcal P$$ 符合某一先验分布，在假设为拉普拉斯和高斯混合分布$$\mathcal P=\{\mu\}, \mu \sim \mathcal M(0, \delta_{0,1}^2, \delta_{0,2}^2)$$时，有：
 $$
\begin{array}{cl}
argmax_{\mu \in (-\infty, +\infty)} f(\mu) &= argmax_{\mu \in (-\infty, +\infty)} \log P(\mathcal O | \mathcal P) P(\mathcal P)\\
& = argmax_{\mu \in (-\infty, +\infty)} \log \left( \prod_{i=1}^n P( x_i \in \mathcal O | \mathcal P) P(\mathcal P)\right)  \\
& = argmax_{\mu \in (-\infty, +\infty)}  \log \left( \left( (\frac{1}{\sqrt{2\pi}})^n e^{-\frac{\sum_{i=1}^n(x_i -\mu)^2 }{2}}\right) 
(2\sqrt{2\pi}\delta_{0,1}^2\delta_{0,2} )^{-1} e^{-\frac{|\mu|}{\delta_{0,1}^2}-\frac{\mu^2}{2\delta_{0,2}^2}} \right)\\
& = -argmax_{\mu \in (-\infty, +\infty)} (\frac{1}{2} \sum_{i=1}^n (x_i -\mu)^2 + n\log \sqrt{2\pi} + \frac{1}{\delta_{0,1}^2} |\mu| + \frac{1}{2\delta_{0,2}^2} \mu^2+ \log (2\sqrt{2\pi}\delta_{0,1}^2\delta_{0,2} ))
\tag{5}\label{prob-b-lap-norm}
\end{array}
$$

## 1.6 统计推断优化求解

寻求最大概率的过程也即求解极大对数似然或极大对数后验估计，对于上述不同的统计推断，有：

求解Prob. \eqref{prob-fre} 等价于求解线性规划问题：
$$
argmin_{\mu \in (-\infty, +\infty)} (\frac{1}{2} \sum_{i=1}^n (x_i -\mu)^2) 
\tag{6}\label{opt-prob-fre}
$$

求解Prob. \eqref{prob-b-norm} 等价于求解岭回归问题：
$$
argmin_{\mu \in (-\infty, +\infty)} (\frac{1}{2} \sum_{i=1}^n (x_i -\mu)^2) + \frac{1}{2 \delta_0^2}\mu^2
\tag{7}\label{opt-b-norm}
$$

求解Prob. \eqref{prob-b-lap} 等价于求解Lasso问题：
$$
argmin_{\mu \in (-\infty, +\infty)} (\frac{1}{2} \sum_{i=1}^n (x_i -\mu)^2) + \frac{1}{\delta_0^2} |\mu| 
\tag{8}\label{opt-b-lap}
$$

求解Prob. \eqref{prob-b-lap-norm} 等价于求解弹性网问题:
$$
argmin_{\mu \in (-\infty, +\infty)} (\frac{1}{2} \sum_{i=1}^n (x_i -\mu)^2) + \frac{1}{\delta_{0,1}^2} |\mu| + \frac{1}{2\delta_{0,2}^2} \mu^2
\tag{9}\label{opt-b-lap-norm}
$$

模型求解见 Sec~\eqref{sec-opt}。



# 二、统计学习

一般学习问题中，核心任务是：假设有观测数据集为 $$\xi =\{(x_i,y_i)\}_{i=1}^n$$， 需要从拟合函数集合$$\mathcal F$$中选取最优拟合函数$$f(x;\mu)=y$$，其中，$$f\in \mathcal F, (x,y)\in \xi, x=[x_1, x_2, \cdots, x_n]$$为观测数据，$$y=[y_1, y_2, \cdots, y_n]$$ 为相应的标签。

## 2.1 学习问题一般范式

学习问题一般可以表示为最小化风险函数。一般先构造损失函数$$l(\mu;\xi)$$为获得最优拟合函数，也即最小化损失函数的期望：

$$
\min_\mu \mathbb R(\mu)=E[l(\mu;\xi)]=\int l(y, f(x;\mu)) dF(x,y),
\tag{10}\label{prob-rm}
$$

其中，$$l(y, f(x;\mu))$$ 为损失函数，$$F(x,y)$$为观测数值对的联合分布。
在实际问题中，联合分布在学习过程中往往未知，直接最小化风险函数并不具有可操作性。于是，最小化风险函数$$R(x)$$一般被替换为最小化经验风险函数$$R^*(x)$$：

$$
\min_\mu R^*(\mu)=\frac{1}{n}\sum_{i=1}^n l(y_i, f(x_i;\mu)) = l(Y, f(X;\mu)),
\tag{11}\label{prob-erm}
$$

其中，$$X=[x_0, x_1, \cdots, x_n],~Y=[y_0, y_1, \cdots, y_n]$$。

## 2.2 线性回归

假设$$f(X;\mu)=X\mu$$，定义损失函数$$l(\mu;\xi) = \frac{1}{2}\|X\mu-Y\|^2$$，
构造线性规划模型，也即：

$$
\min_\mu \frac{1}{2}\|X\mu-Y\|^2.
\tag{12}\label{prob-ml-linear}
$$

可得，$$\mu= (X^\top X)^{-1} X^\top Y$$。
模型求解见 Sec~\ref{sec-opt}。

## 2.3 岭回归

假设$$f(X;\mu)=X\mu$$，定义损失函数$$l(\mu;\xi) = \frac{1}{2}\|X\mu-Y\|^2$$，
同时假设$$\mu$$，添加正则项 $$\frac{\lambda}{2}\|\mu\|^2$$，也即：

$$
\min_\mu \frac{1}{2}\|X\mu-Y\|^2+\frac{\lambda}{2}\|\mu\|^2.
\tag{13}\label{prob-ml-reg}
$$

可得，

$$\mu= (X^\top X + \lambda I)^{-1} X^\top Y$$

这里的$$\lambda I$$也即岭。
模型求解见 Sec~\ref{sec-opt}。

## 2.4 Lasso

假设$$f(X;\mu)=X\mu$$，定义损失函数$$l(\mu;\xi) = \frac{1}{2}\|X\mu-Y\|^2$$，
同时假设模型参数是稀疏的，添加正则项$$\lambda|\mu|$$，也即：

$$
\min_\mu \frac{1}{2}\|X\mu-Y\|^2+\lambda|\mu|.
\tag{14}\label{prob-ml-lasso}
$$

模型求解见 Sec~\ref{sec-opt}。

## 2.5 弹性网络

假设$$f(X;\mu)=X\mu$$，定义损失函数$$l(\mu;\xi) = \frac{1}{2}\|X\mu-Y\|^2$$，
同时添加正则$$\lambda (\frac{\rho}{2}\|\mu\|^2 + (1-\rho) |\mu|)$$，也即：

$$
\min_\mu \frac{1}{2}\|X\mu-Y\|^2 + \lambda \left(\frac{\rho}{2}\|\mu\|^2 + (1-\rho) |\mu| \right).
\tag{15}\label{prob-ml-ela}
$$

模型求解见 Sec~\ref{sec-opt}。


# 三、深度学习

最近，深度学习是学习问题一个备受关注的分支。
深度学习，或深度神经网络模型与传统统计学习有密切联系。从线性回归，经过多层嵌套可以构建一个简单的深度神经网络。
学习问题一般可以形式化为：

$$
\min_\mu l(Y, f(X;\mu)),
$$

线性回归中，$$f(X;\mu) = X\mu$$。

训练过程中，模型参数更新可以表达为： Eq~\ref{SGD}，其中，参数为深度神经网络的模型权重，各个参数的梯度通过反向传播获得。

用 PyTorch 代码实现如下：

```python
import math
import torch
import torch.nn as nn 
import torch.optim as optim
n = 100000
p= 512
x = torch.randn(n,p)
beta = torch.randn(p,1)/math.sqrt(p)
y = torch.mm(X,beta)+0.5 * torch.randn(n,1)
class LinearReg(nn.Module): 
  def __init__(self):
    super(LinearReg,self).__init___() 
    self.linear = torch.nn.Linear(p,1) 
  def forward(self,x) :
    y_pred = self.linear(x) 
    return y_pred

from torch.optim.Ir_scheduler import MultiStepLR
batch_sz = 128
epochs = 9
model = LinearReg()
optimizer = optim.SGD(model.parameters(),lr=0.01,momentum=0.9,weight_decay=5e-4)
scheduler = MultiStepLR(optimizer,milestones=[3,6],gamma=0.2)
Loss = nn.MSELOSS()
for epoch in range(epochs):
  scheduler.step()
  for idx in range(n // batch_sz):
    min idx = batch_sz*idx
    max idx = batch_sz*idx+batch_sz
    x_batch,y_batch = X[min_idx;max_idx,:],y[min_idx:max_idx]
    optimizer.zero_grad()
    Loss(model(x_batch),y_batch).backward() 
    optimizer.step()
```



深度学习中，前向全连接神经网络为:

$$f(X;\mu) = W_3 \delta( W_2 \delta( W_1 \mu) ),$$

$$\delta(x)=max(x,0).$$

模型相应的代码更改为：
```python
import torch.nn.functional as F
class Net (nn.Module):
  def __init_(self):
     super(Net,self)._init__()
	 self.fc1 = nn.Linear(p,p)
	 self.fc2 = nn.Linear(p,p)
	 self.fc3 = nn.Linear(p,1)
  def forward(self,x):
    x = F.relu(self.fcl(x))
	x = F.relu(self.fc2(x))
	x = self。fc3(x) 
	return x
```


# 四、最优化方法

$$\tag{sec-opt}\label{sec-opt}$$

最优化方法关注于最小化目标函数:

$$
\min_x f(x),
$$

其中，决策变量$$x \in \mathbb R^{M}$$。

实际问题中，最优化问题$$\min_x f(x)$$基本没有直接的闭式解析解，一般采用迭代求解方式得到原问题最优解：

$$
x^{k+1} = x^k - \nabla f(x^k)
$$

## 3.1 随机梯度下降法

在大规模数据集驱动的机器学习问题中，数据集$$A \in \mathbb R^{N \times M}$$的数量很大，维度很高，带来巨大的内存开销，使得原始的全梯度下降法无法在普通计算平台上运行。
为了解决这类问题，一般构造随机梯度下降算法，每次仅用小批量数据$$\{\xi_i=(a_i,b_i)\}$$来更新模型参数 x：

$$
x^{k+1} = x^k - \nabla f(x^k;\xi_i)
\tag{SGD}\label{SGD}
$$

其中，$$\{\xi_i=(a_i,b_i)\}$$为某一小批量数据。典型的深度学习BP算法也即是使用SGD来更新模型参数的。

## 3.2 更多优化求解算法（使用ADMM求解稀疏学习问题）
 
接下来，以稀疏学习为例进行优化求解分析。

令数据集$$A \in \mathbb R^{N \times M}$$为观测数据，
$$b \in \mathbb R^{N}$$ 为相应的标签。
在实际应用场景中，通常有$$M \gg N$$
稀疏学习假设模型参数是稀疏的，选取最为稀疏的模型，也即求解稀疏线性回归问题：

$$
\min_{x \in \mathbb R^M} \left[ f(x) := \frac{1}{2} \|Ax-b\|^2 + \mu\|x\|_1\right].
\tag{prob-min}\label{prob-min}
$$

可以使用变方向乘子法（ADMM）来求解 Prob. \eqref{prob-min}。
首先，引入附属变量$$y$$将原问题 Prob. \eqref{prob-min} 重新形式化为：

$$
\begin{array}{ccl}
& \min_{x,y} & \frac{1}{2} \|Ax-b\|^2 + \mu\|y\|_1 \nonumber\\
& s.t. & y-x =0,
\end{array}
\tag{prob-min-xy}\label{prob-min-xy}
$$

然后应用 ADMM 算法框架求解问题 Prob. \eqref{prob-min-xy}，算法可以描述为：

$$
\begin{array}{cl}
x^{k+1} & := &argmin_{x} \ \mathcal L_\beta\left(x,y^k,{\lambda}^k\right), \\
y^{k+1} & := &argmin_{z} \ \mathcal L_\beta\left(x^{k+1},y,{\lambda}^k\right),\\
{\lambda}^{k+1} & := & {\lambda}^k - \beta\left(x^{k+1} - y^{k+1}\right),
\end{array}
$$

其中，增广拉格朗日函数$$\mathcal L_\beta(x,y,{\lambda})$$定义为：

$$
\mathcal L_\beta\left(x,y,{\lambda}\right) = l(x) + \mu \|y\|_1 - \left\langle {\lambda}, x-y \right\rangle + \frac{\beta}{2}\left\|x-y\right\|^2.
$$

其中，$$\lambda$$是拉格朗日乘子， $$\beta>0$$ 是惩罚参数。
在该问题中，有解析表达式：

$$
x^{k+1} = \left(\beta I + A^\top A \right)^{-1} \left[ A^\top b + \beta y^k -\lambda^k \right],
$$

和

$$
y^{k+1} = sign \left(x^{k+1}+\frac{\mu \lambda^k}{\beta} \right) \bullet \max \left\{ \left| x^{k+1} + \frac{\mu \lambda^k}{\beta} \right| -  \frac{\mu}{\beta}, 0  \right\}.
$$



