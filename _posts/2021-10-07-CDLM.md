---
layout: post
mathjax: true
catalog: true
comments: true
top-tags-list: true
header-img: "img/post-bg-universe.jpg"
header-mask: 0.4
title: 跨文档语言模型
subtitle: CDLM Cross-Document Language Modeling
author: 阚志刚
tags: [预训练语言模型, 跨文档, Longformer]
---

# 一、 简介

本文是来自以色列巴伊兰大学计算机系、艾伦人工智能研究所和华盛顿大学计算机科学与工程学院的工作[网页传送](hhttps://arxiv.org/pdf/2101.00406.pdf)，文章发表在EMNLP 2021上。

# 二、 动机 & 贡献

## 1、动机

现在的预训练语言模型（LM）基本上都是将文章独立处理的，然而实际上很多任务是跨文档的，需要模型具备捕捉跨文档信息的能力。

## 2、贡献

### 1）提出了一种面向跨文档任务（CD）的预训练方法

在训练的时候使用一系列内容比较相关的文章作为预训练语言模型的输入。

### 2）面向跨文档的任务，提出了一种动态全局注意力机制

根据跨文档任务的特点，在Longformer的基础上对关键token使用全局注意力，使其能够关注到全局信息。

# 三、 方法

## 1、Longformer

本文模型基本上是基于Longformer这样一个能够捕捉文档级信息的预训练语言模型来做改进的，Longformer的详细信息可以参考博客[Longformer传送门](https://wmathor.com/index.php/archives/1509/)

## 2、跨文档语言模型

### 1）在内容相关的文档中训练

在实际任务中，有一些文章描述了相同的事情，它们包含了重合的信息。跨文档的预训练语言模型在预训练的过程中可以利用不同文章中的信息来预测被遮住的token。输入信息如下图所示：

{% include figure.html src="/figures/2021-10-07-CDLM/CDLM pretraining.png" caption="Input of CDLM"%}

CDLM的输入是多篇文章，文章的开头和结尾使用"<doc-s>"和"</doc-s>"字符来划分文章的界限。CDLM的基本设置继承CDLM，最长可以编码4096个字符。跨文档训练时的数据集是Multi-News dataset（一种款文档的任务的数据集）

### 2) 使用全局注意力进行预训练

模型的基本框架继承于Longformer，预训练过程中使用BERT中的预测遮掩单词的方法，将输入中的15%的单词遮掩住，让模型来预测它们。为了使被遮掩的单词能够感受到全局信息，作者给它们赋予了全局注意力。

# 四、实验

## 1、内部性能实验

目的是证明模型预训练过程的效果。结果如下：

{% include figure.html src="/figures/2021-10-07-CDLM/e1.png" caption="实验1结果"%}

## 2、跨文档共指解析

这个实验给我们提供了一个该如何运用CDLM的示例。这个任务是判断两个处于不同文档的实体或者事件是否相同。之前的SOTA工作的做法是基于聚合层次聚类（Agglomerative Hierarchical Clustering），将两个实体（事件）所在的句子（经LM后）的编码拼接起来，送入MLP获取一个得分，根据这个得分来判断两者是否共指。

而使用CDLM的话，跨文档共指解析的任务就可以换一种方式来实现，如下：

{% include figure.html src="/figures/2021-10-07-CDLM/overview1.png" caption="跨文档共指解析任务的模型示意。"%}

将两个待处理的实体（事件）所在的文章作为模型的输入，对"[CLS]"以及实体（事件）提及字符赋予全局注意力。经过CDLM编码之后，取上述字符的编码进行求和、乘积、拼接等操作得到特征表示，最后与其他工作一样，使用聚合层次聚类来聚合最相似的簇。下面是实验结果：

{% include figure.html src="/figures/2021-10-07-CDLM/e2.png" caption="跨文档共指解析实验结果"%}

## 3、其他实验

实验设计上跟跨文档共指解析实验差不多，不多赘述了。