---
layout: post
mathjax: true
catalog: true
comments: true
top-tags-list: true
header-img: "img/post-bg-universe.jpg"
header-mask: 0.4
title: Capuchin论文阅读
author: 杨智琳
tags: [内存管理]
---

# Capuchin: Tensor-based GPU Memory Management for Deep Learning

​	本文提出了一种通过tensor的eviction/prefetching和重计算来减少内存占用的GPU内存管理模块——Capuchin。它的关键特点是，它是基于在运行时追踪到的动态tensor访问模式来做出内存管理的决策。

## 一、背景

#### （一）基于计算图的静态分析的三个问题：

​		1）硬件和输入大小的异构性使得预测计算时间很困难，即使对于相同类型的层，计算时间也相差很大。

​		2）“定性”的信息不能量化特定内存操作的开销，难以对内存优化候选对象进行优先级排序，难以在swapping和重新计算之间做出选择。

​		3）DNN发展较为快速，对于一些新型的DNN网络，目前还没有先验知识。此外，基于计算图的内存管理对于执行前没有计算图的框架不适用，不具有普适性。

#### （二）深度学习框架执行模式：

​		1）Eager Mode：命令式编程环境会立即计算操作，而无需构建图。

​		2）Graph Mode：计算图在执行开始之前建立，实际计算只在必要时安排。

## 二、动机

#### （一）静态分析的局限性：

​		**swapping中的同步开销：**如下图的例子可以看到，swapping in/out的时间比overlapped层的执行时间多3倍。当当前层的执行时间不足以overlap数据传输时，同步开销就会非常大。

{% include figure.html src="/figures/Capuchin/figure1.png" caption="Synchronization Overhead of vDNN on Vgg16."%}

​		**同类型层的执行时间变化：**不同卷积层的执行时间变化很大，会使得overlap变得困难；忽视CNN层的重计算不利于内存优化。

#### （二）tensor的访问模式：

​		tensor的遵循一个规律的访问模式，即在一次迭代中出现的次数和时间戳大多是固定的。借此可以识别出哪些tensor可用于减少内存占用；其次，tensor的访问时间间隔有助于做出内存管理决策。如下图，对T1进行连续两次访问的时间间隔更大，对T1进行swap更有益于减少开销。

{% include figure.html src="/figures/Capuchin/figure2.png" caption="ResNet-50 Tensor Accesses Timeline."%}

## 三、Capuchin内存管理机制

#### （一）设计目标：

​		1）尽量减少开销；

​		2）代码修改应该尽量少，框架应该较为通用。

#### （二）设计概述：

​		训练的两个阶段：

​		1）测试阶段：执行第一个mini-batch，并从中观察到动态tensor的访问序列特征；

​		2）执行阶段：在后续训练过程中，基于第一轮迭代观察到的tensor访问模式进行后续内存管理的决策。

#### （三）估计swap和recomputation的效益

​		对于swap，应该增加swap和computation之间的overlap；对于recomputation，应该尽量选择开销小的操作来进行重计算。

​		**Swap：**SwapTime可以通过将tensor的内存大小除以PCIe的带宽来计算。用Free Time (FT) 来进行量化：

{% include figure.html src="/figures/Capuchin/figure3.png" caption=""%}

​		**Recomputation：**对于recomputation，需要估计重计算操作的成本，定义了Memory Saving Per Second (MSPS) 来衡量重新计算一个tensor的效益，即内存节省的越多、重新计算时间越少MSPS越高。

{% include figure.html src="/figures/Capuchin/figure4.png" caption=""%}

#### **（四）确定tensor重新生成的开销**

​		**目标：**选择合适的时间来进行重新生成操作。

​		**对于swap：**可以通过计算SwapInStartTime（back-access time 减去 SwapTime）。不过当前内存占用如果比较严重，是不适合进行生成的，否则可能会造成其他tensor的eviction。本文引入了一种反馈驱动调整机制，就是当in-trigger时间应该调整的更早时，那么下一次迭代训练会动态调整把这个in-trigger时间提前。

{% include figure.html src="/figures/Capuchin/figure5.png" caption=""%}

​		**对于recomputation：**recomputation会占用计算资源，我们选择在需要的时候再进行重计算。在选择通过recomputation重新生成的tensor时，需要计算当前候选集中所有tensor的MSPS，但是一旦选择出一个tensor插入到eviction集中，将导致候选集中其他张量的MSPS变化，需要重新计算他们的MSPS。其算法改进如下所示：

{% include figure.html src="/figures/Capuchin/figure6.png" caption=""%}

#### （五）选择内存优化方案

​		由于swap可以在很大程度上与计算overlap，而重计算会消耗计算资源。因此优先选择swap。算法流程如下图所示：

​		首先满足以下条件的tensor将会被加入eviction候选集中:（1）tensor的访问次数大于1（2）tensor的访问发生在内存使用高峰期时段。然后先根据FT从大到小做一个排序，FT越大的tensor越值得去进行swap或者recompute。然后将recomputation开销与swap该张量的开销进行比较，选择开销较小操作。

{% include figure.html src="/figures/Capuchin/figure7.png" caption=""%}

## 四、Capuchin的实现：

#### （一）对底层框架的需求：

​			**Executor：**采用了一种on-the-fly lineage-based的重计算，类似于Spark的RDD lineage。

​			**Allocator：**它是动态GPU分配原始方法的包装器，通常为高层模块提供两种方法，Allocate和Deallocate。为了支持swapping，allocator支持另外两种方法，即SwapOut和SwapIn。

####  （二）系统结构：

​		**Tensor的结构：**

{% include figure.html src="/figures/Capuchin/figure8.png" caption=""%}

​		**Capuchin的两个模块：**

​			**1）Tensor Access Tracker (TAT)：**支持按需内存交换以解决OOM和访存失败的问题；追踪tensor的访问模式来使PM可以做出内存优化决策。

​				**按需内存交换：**如下图所示，当发生OOM时，TAT将从头开始在访问列表中寻找一个或多个大小合适的tensor进行evict。然后，将被evict的tensor同步复制到CPU中，并将{tensor_id，cpu_addr}保存在TAT中。之后如果再对被evict的tensor进行访问，将通过对应的CPU地址将data复制到GPU中去。

{% include figure.html src="/figures/Capuchin/figure9.png" caption="Passive Mode of Tensor Access Tracker."%}

​			**2）Policy Maker (PM)：**PM根据TAT提供的tensor访问信息，做出内存管理策略的决策。

#### （三）优化：

​			**1）computation和swapping的解耦：**

​				问题：如下图左图所示，当当前的计算不足以overlapping swapping-out时，同步数据传输的开销很大。

​				优化方法：在一个tensor进行swapping-out 时解耦计算和数据传输，并且只在发生OOM时同步最早未完成的swapping-out。

{% include figure.html src="/figures/Capuchin/figure10.png" caption="Decoupled Computation and Swapping."%}

​			**2）collective recomputation：**在重计算T4的过程中，一旦T2计算出来，将保留T2 。继续计算T4时，如果内存足够，仍保留T2，否则将内存释放。这样就可以在一个重计算过程中尽可能多的保留最新的重计算tensor。



​		

























































