---
layout: post
title: 时序知识图谱事件预测总结
comments: True
author: gaoIf
---
作者：高翊夫

# 背景知识
时序知识图谱预测分为补全(interpolation)和外推(extrapolation)，补全是推理当前时间范围内的实体，例如给定时间段($t_1-t_n$),推理出$t_T$($t_1<t_T<t_n$)时刻未出现的实体；外推是预测未来时间的($t_T>t_n$)事件出现。
一般来说时序图可以理解为动态图的特殊情况。在补全设定下，此时时序知识图谱也可以退化成**图的动态变化**(dynamic of graph)，此时的研究人员关注点在于连续的动态节点表示，也就是说研究图上节点随时间变化的动态嵌入表示。代表工作有TA-DisMult[<sup>1</sup>](#refer-anchor-1),TTransE[<sup>2</sup>](#refer-anchor-2)等，将事实出现时刻融入到关系的嵌入中。
在外推设定下，此时时序知识图谱可以说成**动态变化的图**(dynamic on graph),这是将时序知识图谱可以看做是离散的随时间变化的图快照序列，研究者更加关注每个时间节点上的图结构依赖以及不同时刻图之间的时序关系，这是可以说是时序知识图谱。
本课题也主要是关注在时序知识图谱下对未来事实出现的预测，也就是外推情况下的推理，这里需要注意的是时间点过程虽然关注于连续动态节点嵌入，但其主要用于外推推理，下图为背景知识的概念图：
{% include figure.html  width="488" src="/figures/时序知识图谱/图1.jpeg" %}


# 一. 基于时间点过程
## 1. 背景介绍
在数学上，人们可以将序列（流）中的事件在某些方面视为独立的或相关的。旨在描述此类事件流的数学领域称为点过程。事件序列的一种最基本的模型是泊松过程，它假设事件是相互独立的，即一个事件的发生根本不会影响其他事件的发生。表征时间点过程的一种重要方法是通过条件强度函数 λ(t)，这是给定所有先前事件的下一个事件时间的随机模型。强度函数 λ(t) 表示单位长度区间内事件的预期数量。λ(t)dt 是在给定历史的小窗口 [t, t + dt) 中观察事件的条件概率。根据生存分析理论，给定历史T={t1, . . . , tn}，对于任何 t > tn，我们将在 [tn, t) 期间没有事件发生的条件概率表征为
{% include figure.html  width="288" src="/figures/时序知识图谱/公式.png" %}
因此，定义事件在时间 t 发生的条件密度
{% include figure.html  width="288" src="/figures/时序知识图谱/公式2.png" %}
不同强度函数可以代表不同的点过程。
## 2. Know-Evolve: Deep Temporal Reasoning for Dynamic Knowledge Graphs Rakshit（ICML 2017）
该篇文章率先采用点过程去解决外推推理问题。其将点过程的强度函数表示成动态实体嵌入，具体表示如下所示：
{% include figure.html  width="288" src="/figures/时序知识图谱/公式3.png" %}
{% include figure.html  width="288" src="/figures/时序知识图谱/公式4.png" %}
可以发现时间点过程的强度函数通过动态实体表示学习到，该方法存在无法处理共现事件的问题。

## 3. DYREP: LEARNING REPRESENTATIONS OVER DYNAMIC GRAPHS （ICLR 2019）
DyRep是 KnowEvolve后继工作。 DyRep 通过使用 TPP 对长期事件（拓扑演化）和短期事件（节点通信）进行建模并引入聚合机制来扩展 Know-Evolve。简而言之，动态更新节点使采用聚合通信的方式更新表示。

## 4. Learning Representation over Dynamic Graph using Aggregation-Diffusion Mechanism（2021）
该文章发现时间点过程(Dyrep)仅依靠聚合来传播动态图中的信息会导致信息传播的延迟，从而影响方法的性能。为了缓解这个问题，该文提出了一种聚合扩散（AD）机制，在节点通过聚合机制更新其嵌入后，通过扩散主动将信息传播到其邻居。AD机制率先采用在补全情况收集节点的过去历史信息，这里作者将其利用在时间点过程中，并扩展了Dyrep方法。

## 5. Graph Hawkes Neural Network for Forecasting on Temporal Knowledge Graphs(2021)

GHNN是另一种基于TPP的方法，它使用适用于Hawkes过程的连续时间LSTM。与 Know-Evolve 类似，GHN专为知识图谱而设计。
### 5.1 Hawkes过程
Hawkes过程是点过程中的另一种数学模型，它假设过去的事件会影响未来事件发生产生积极的概率，并模拟过去事件对未来事件的影响。Neural Hawkes Process [<sup>3</sup>](#refer-anchor-3)于 2017 年首次提出，旨在通过构建“神经自调制多元点过程”来扩展传统的霍克斯过程，即将 LSTM 引入计算事件强度 λ 的过程。 这种设置省略了先前对强度非负性的限制，允许使用模型描述“自我抑制”事件。下图为模型机制介绍图：
{% include figure.html  width="488" src="/figures/时序知识图谱/图2.png" %}
利用LSTM神经网络去学习强度函数λ有以下好处：1）可以摆脱过去事情只有正激励过程，而没有抑制情况。 2）可以违反指数衰减假设以涵盖更复杂的延迟衰减情况或在两个事件的时间间隔内激发/抑制切换的情况。Neural Hawkes Process针对的是连续时间序列问题。

### 5.3 GHNN过程

GHNN过程就是针对时序知识图谱问题，基于Neural Hawkes Process方法，用于对连续时间的离散大规模多关系图序列进行建模。其主要区别是在于两个模块：1）邻域聚合模块，用于从发生在同一时间戳的并发事件中捕获信息。2）Graph Hawkes 过程，用于对未来事实的发生进行建模，其中我们使用循环神经网络来学习这个时间点过程。

## 6. Event-centric Forecasting on Temporal Knowledge Graph with Transformer
该方法主要是基于GHNN，将GHNN用RNN类网络学习Hawkes过程，改善成用Transformer，并做敏感性实验讨论，结果发现在特定结构的Transformer表现不属于LSTM，其余的效果不好。还提出了改进方形，在聚合函数方面后者采用更新的informer[<sup>4</sup>](#refer-anchor-4)架构去尝试长依赖序列结构。
<div id="refer-anchor-1"></div>
[1] Learning sequence encoders for temporal knowledge graph completion.
<div id="refer-anchor-2"></div>
[2] Deriving validity time in knowledge graph.
<div id="refer-anchor-3"></div>
[3] The neural hawkes process: A neurally selfmodulating multivariate point process.
<div id="refer-anchor-4"></div>
[4] Informer: Beyond eﬃcient transformer for long sequence time-series forecasting

# 二. 基于离散图快照结构