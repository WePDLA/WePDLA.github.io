---
layout: post
title: 时序知识图谱事件预测总结
comments: True
author: gaoIf
---
作者：高翊夫

# 背景知识
时序知识图谱预测分为补全(interpolation)和外推(extrapolation)，补全是推理当前时间范围内的实体，例如给定时间段($t_1-t_n$),推理出$t_T$($t_1<t_T<t_n$)时刻未出现的实体；外推是预测未来时间的($t_T>t_n$)事件出现。
一般来说时序图可以理解为动态图的特殊情况。在补全设定下，此时时序知识图谱也可以退化成**图的动态变化**(dynamic of graph)，此时的研究人员关注点在于连续的动态节点表示，也就是说研究图上节点随时间变化的动态嵌入表示。代表工作有TA-DisMult[<sup>1</sup>](#refer-anchor-1),TTransE[<sup>2</sup>](#refer-anchor-2)等，将事实出现时刻融入到关系的嵌入中。
在外推设定下，此时时序知识图谱可以说成**动态变化的图**(dynamic on graph),这是将时序知识图谱可以看做是离散的随时间变化的图快照序列，研究者更加关注每个时间节点上的图结构依赖以及不同时刻图之间的时序关系，这是可以说是时序知识图谱。
本课题也主要是关注在时序知识图谱下对未来事实出现的预测，也就是外推情况下的推理，这里需要注意的是时间点过程虽然关注于连续动态节点嵌入，但其主要用于外推推理，下图为背景知识的概念图：
{% include figure.html  width="488" src="/figures/时序知识图谱/图1.jpeg" %}


# 一. 基于时间点过程
## 1. 背景介绍
在数学上，人们可以将序列（流）中的事件在某些方面视为独立的或相关的。旨在描述此类事件流的数学领域称为点过程。事件序列的一种最基本的模型是泊松过程，它假设事件是相互独立的，即一个事件的发生根本不会影响其他事件的发生。表征时间点过程的一种重要方法是通过条件强度函数 λ(t)，这是给定所有先前事件的下一个事件时间的随机模型。强度函数 λ(t) 表示单位长度区间内事件的预期数量。λ(t)dt 是在给定历史的小窗口 [t, t + dt) 中观察事件的条件概率。根据生存分析理论，给定历史T={t1, . . . , tn}，对于任何 t > tn，我们将在 [tn, t) 期间没有事件发生的条件概率表征为
{% include figure.html  width="288" src="/figures/时序知识图谱/公式.png" %}
因此，定义事件在时间 t 发生的条件密度
{% include figure.html  width="288" src="/figures/时序知识图谱/公式2.png" %}
不同强度函数可以代表不同的点过程。
## 2. Know-Evolve: Deep Temporal Reasoning for Dynamic Knowledge Graphs Rakshit（ICML 2017）
该篇文章率先采用点过程去解决外推推理问题。其将点过程的强度函数表示成动态实体嵌入，具体表示如下所示：
{% include figure.html  width="288" src="/figures/时序知识图谱/公式3.png" %}
{% include figure.html  width="288" src="/figures/时序知识图谱/公式4.png" %}
可以发现时间点过程的强度函数通过动态实体表示学习到，该方法存在无法处理共现事件的问题。

## 3. DYREP: LEARNING REPRESENTATIONS OVER DYNAMIC GRAPHS （ICLR 2019）
DyRep是 KnowEvolve后继工作。 DyRep 通过使用 TPP 对长期事件（拓扑演化）和短期事件（节点通信）进行建模并引入聚合机制来扩展 Know-Evolve。简而言之，动态更新节点使采用聚合通信的方式更新表示。

## 4. Learning Representation over Dynamic Graph using Aggregation-Diffusion Mechanism（2021）
该文章发现时间点过程(Dyrep)仅依靠聚合来传播动态图中的信息会导致信息传播的延迟，从而影响方法的性能。为了缓解这个问题，该文提出了一种聚合扩散（AD）机制，在节点通过聚合机制更新其嵌入后，通过扩散主动将信息传播到其邻居。AD机制率先采用在补全情况收集节点的过去历史信息，这里作者将其利用在时间点过程中，并扩展了Dyrep方法。

## 5. Graph Hawkes Neural Network for Forecasting on Temporal Knowledge Graphs(2021)

GHNN是另一种基于TPP的方法，它使用适用于Hawkes过程的连续时间LSTM。与 Know-Evolve 类似，GHN专为知识图谱而设计。
### 5.1 Hawkes过程
Hawkes过程是点过程中的另一种数学模型，它假设过去的事件会影响未来事件发生产生积极的概率，并模拟过去事件对未来事件的影响。Neural Hawkes Process [<sup>3</sup>](#refer-anchor-3)于 2017 年首次提出，旨在通过构建“神经自调制多元点过程”来扩展传统的霍克斯过程，即将 LSTM 引入计算事件强度 λ 的过程。 这种设置省略了先前对强度非负性的限制，允许使用模型描述“自我抑制”事件。下图为模型机制介绍图：
{% include figure.html  width="488" src="/figures/时序知识图谱/图2.png" %}
利用LSTM神经网络去学习强度函数λ有以下好处：1）可以摆脱过去事情只有正激励过程，而没有抑制情况。 2）可以违反指数衰减假设以涵盖更复杂的延迟衰减情况或在两个事件的时间间隔内激发/抑制切换的情况。Neural Hawkes Process针对的是连续时间序列问题。

### 5.2 GHNN过程

GHNN过程就是针对时序知识图谱问题，基于Neural Hawkes Process方法，用于对连续时间的离散大规模多关系图序列进行建模。其主要区别是在于两个模块：1）邻域聚合模块，用于从发生在同一时间戳的并发事件中捕获信息。2）Graph Hawkes 过程，用于对未来事实的发生进行建模，其中我们使用循环神经网络来学习这个时间点过程。

## 6. Event-centric Forecasting on Temporal Knowledge Graph with Transformer
该方法主要是基于GHNN，将GHNN用RNN类网络学习Hawkes过程，改善成用Transformer，并做敏感性实验讨论，结果发现在特定结构的Transformer表现不属于LSTM，其余的效果不好。还提出了改进方形，在聚合函数方面后者采用更新的informer[<sup>4</sup>](#refer-anchor-4)架构去尝试长依赖序列结构。


# 二. 基于离散图快照
基于离散图快照序列的外推推理，可以说是真正意义上的时序知识图谱。其中心思想是将每个时刻的事实构建出图谱结构，（subj，relation，obj）可以看做一个历史事件。一般研究人员会采用聚合架构来对同一时刻共现事件建模，采用时序模型对不同时刻时序图联系建模。
## 1. Recurrent Event Network: Autoregressive Structure Inference over Temporal Knowledge Graphs（2020 EMNLP）
本文是率先采用RNN等循环网络编码事件，并且采用聚合函数对相同时刻事实联系进行建模。
### 1.1 模型动机
先前研究（Know-evolve等）在没有先前事件的基本事实的情况下，无法在未来的时间戳上顺序预测事件；并且无法对在同一时间窗口的共现事件进行建模。因此RE-NET关键思想基于以下事实：

* 时间上相邻的事件可能携带相关的语义和信息模式，这可以进一步帮助预测未来事件（即时间信息）
* 多个事件可能在同一时间窗口内同时发生，并表现出实体之间的结构依赖性

具体来说，RE-NET采用循环事件编码器汇总过去事件序列的信息，邻域聚合器汇总同一时间窗口内并发事件的信息。 使用汇总信息，利用解码器定义当前事件的联合概率。 

### 1.2 模型细节

首先，未来时刻尾实体o预测概率可以通过当前时刻隐藏状态输入MLP后得到，具体实现如下所示：
{% include figure.html  width="200" src="/figures/时序知识图谱/公式5.png" %}
该概率实际上是经过softmax得到，因此正比于指数函数。es，er为当前时刻的实体嵌入，ht-1为当前时刻的隐藏状态，聚合了有关s的过去历史信息。

ht-1为隐藏状态矩阵，具体是由RNN网络得到，如下公式所示：
{% include figure.html  width="200" src="/figures/时序知识图谱/公式6.png" %}
g函数采用了RGCN方法，汇聚了局部邻域信息；Ht是预训练好的带有全局图信息的隐藏状态矩阵。RGCN聚合公式如下图所示：
{% include figure.html  width="200" src="/figures/时序知识图谱/公式7.png" %}

## 2. Learning from History: Modeling Temporal Knowledge Graphs with Sequential Copy-Generation Networks（2020 AAAI）

### 2.1 模型动机
作者发现事件的80%以上已经出现在前一个时间段内。 这种现象凸显了利用已知事实预测未来事实的重要性。简而言之利用历史事实的重复性来预测未来事件出现。

### 2.2 模型细节

CyGNet 由两种推理模式组成，分别是 Copy 模式和 Generation 模式，利用Copy模式从已知实体词汇表中预测实体概率，采用Generation模式从整个实体词汇表中推断出实体概率。 然后，将两个概率分布结合起来作为最终预测。

## 3. Temporal Knowledge Graph Reasoning Based on Evolutional Representation Learning（2021 SIGIR）

### 3.1 模型动机
时序知识图谱存在同一时刻共现事件的结构依赖性和时序链接性，其动机与第一篇差不多，但在具体实现细节上有所简化以及提高。原有的RENET存在以下不足：
(1) 主要关注给定查询的实体和关系，忽略每个时间戳的KG中所有事实之间的结构依赖性； 
(2) 对每个查询单独编码历史，效率低下；
(3) 忽略实体类型等实体的一些静态属性的功能。 此外，现有方法只关注实体预测，而关系预测无法通过同一模型同时解决。

### 3.2 模型细节
模型细节图如下所示：
{% include figure.html  width="488" src="/figures/时序知识图谱/图3.png" %}
与RENET区别：
（1） 使用新的relation-aware GCN而不是RGCN聚合同一时刻图谱里的邻域信息，具体区别如下所示
{% include figure.html  width="200" src="/figures/时序知识图谱/公式8.png" %}。
（2） RE-GCN类似使用的是RE-NET的全局信息Ht，在时序编码上没有再对每个查询编码历史的局部信息，并且添加了两个门控单元，作者意思是可以使时序模型更加支持长依赖。
（3）添加了静态图属性的实体编码。
（4）采用了ConvTransE解码器，可以对实体和关系隐藏状态矩阵进行解码预测实体和关系。


## 4. HIP Network: Historical Information Passing Network for Extrapolation Reasoning on Temporal Knowledge Graph（2021 IJCAI）

### 4.1 模型动机
对复杂的共现模式以及时序联系建模。

### 4.2 模型细节
（1）大致上说，在聚合函数时采用多头comggcn有选择的更新邻域节点信息，在时序上 实体更新采用self-attention替代RNN等自回归结构。在时序self-attention实现的过程中采取了对未来信息mask的方式，使得传播过程中只能看到过去信息。关系更新采用rnn架构，该模型也是只考虑全局更新信息，没有对每个查询编码历史局部信息。
（2）在预测时候提出了一个新的多步推理的方式来生成预测图结构。具体来说，多步推理中采用了三种分数来计算。首先使用时序性分数来得到未来时刻的候选四元组（使用了时序更新的实体和关系表示），然后通过结构性分数（只是用comggcn更新的实体和关系表示）和重复性分数结合预测出最终的目标。

## 5. Dynamic Knowledge Graph based Multi-Event Forecasting（KDD 2020)
本文的创新点是对于每一个三元组事实，有一个摘要概述，将同一时刻的所有事实摘要概述构成一个词图，节点为词，边为PMI权重，然后通过GCN训练隐藏状态表示，最终预测的时候利用这个隐藏状态表示和时序图的表示结合成新的隐藏状态进行预测；除此之外，也添加了关系预测内容。
### 5.1 模型动机
现有工作中识别的证据在预测多个不同类型的并发事件时往往难以理解或需要进一步的人工检查。 鉴于过去事件的复杂联系，很难为每种事件类型识别正确的参与者和线索。 主要解决多事件和多参与者的预测任务，抽象成两个多分类的问题：1.预测共现事件的多种事件类型 2.预测给定事件类型的多个参与者。
### 5.2 模型细节
本文通过考虑两个相互关联的子问题来解决上述挑战：预测不同类型的多个同时发生的事件并推断每个事件类型中的多个参与者。 我们提出了一种新的方法来融合来自异构历史数据的关系（事件图）和语义（文本）信息，以预测未来多种类型和多个参与者的并发事件。 
{% include figure.html  width="200" src="/figures/时序知识图谱/公式9.png" %}
{% include figure.html  width="200" src="/figures/时序知识图谱/公式10.png" %}。
## 6. Search from History and Reason for Future: Two-stage Reasoning on Temporal Knowledge Graphs(ACL 2021)
### 6.1 模型动机
### 6.2 模型细节

<div id="refer-anchor-1"></div>
[1] Learning sequence encoders for temporal knowledge graph completion.
<div id="refer-anchor-2"></div>
[2] Deriving validity time in knowledge graph.
<div id="refer-anchor-3"></div>
[3] The neural hawkes process: A neurally selfmodulating multivariate point process.
<div id="refer-anchor-4"></div>
[4] Informer: Beyond eﬃcient transformer for long sequence time-series forecasting.